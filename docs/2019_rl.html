<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">

    <title>Reinforcement Learning</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes"/>
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"/>

    <meta name="viewport"
          content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

          <link rel="stylesheet" href="reveal.js/css/reveal.css">
          <!--<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/night.css" id="theme">-->
          <!--<link rel="stylesheet" href="reveal.js/css/theme/simple.css" id="theme">-->
          <link rel="stylesheet" href="reveal.js/css/theme/solarized.css" id="theme">
      
          <!-- Code syntax highlighting -->
          <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
          <style>
              /*pre code {*/
                  /*display: block;*/
                  /*padding: 0.5em;*/
                  /*background: #FFFFFF !important;*/
                  /*color: #000000 !important;*/
              /*}*/
      
              .right-img {
                  margin-left: 10px !important;
                  float: right;
                  height: 500px;
              }
              .todo:before {
                  content: 'TODO: ';
              }
              .todo {
                  color: red !important;
              }
              code span.line-number {
                  color: lightcoral;
              }
              .reveal pre code {
                  max-height: 1000px !important;
              }
      
              img {
                  border: 0 !important;
                  box-shadow:0 0 0 0 !important;
              }
      
              .reveal {
                  -ms-touch-action: auto !important;
                  touch-action: auto !important;
                      }
      
                      .reveal h2,
                      .reveal h3,
                      .reveal h4 {
                        letter-spacing: 2px;
                          font-family: 'Calibri', sans-serif;
                          /* font-family: 'Times New Roman', Times, serif; */
                          font-weight: bold;
                          color: black;
                          font-style: italic;
                          letter-spacing: -2px;
                          text-transform: none !important;
                      }
      
                      .reveal em {
                          font-weight: bold;
                      }
      
                      .reveal .step-subtitle h1 {
                          letter-spacing: 1px;
                      }
                      .reveal .step-subtitle h2,
                      .reveal .step-subtitle h3 {
                          text-transform: none;
                          font-style: italic;
                          font-weight: normal;
                          /* font-weight: 400; */
                          /* font-family: 'Amiri', serif; */
                          font-family: 'Lobster', serif;
                          letter-spacing: 1px;
                          color: #2aa198;
                          text-decoration: underline;
                      }
      
                      .reveal .front-page h1,
                      .reveal .front-page h2 {
                          font-family: "League Gothic";
                          font-style: normal;
                          text-transform: uppercase !important;
                          letter-spacing: 1px;
                      }
      
                      .reveal .front-page h1 {
                          font-size: 2.5em !important;
                      }
      
                      .reveal .highlight {
                          background-color: #D3337B;
                          color: white;
                      }
      
              .reveal section img {
                background: none;
              }
      
                      .reveal img.with-border {
                          border: 1px solid #586e75 !important;
                          box-shadow: 3px 3px 1px rgba(0, 0, 0, 0.15) !important;
                      }
      
                      .reveal li {
                          margin-bottom: 8px;
                      }
      
                      /* For li's that use FontAwesome icons as bullet-point */
                  .reveal ul.fa-ul li {
                      list-style-type: none;
                  }

            .reveal {
                color: black !important;
             }       

          </style>
      
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>

    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->
</head>

<body>

<div class="reveal">
    <div class="slides">

<section data-markdown class="preparation">
        <textarea data-template>
### Preparation

    </textarea>
</section>

<section>
        <h2>Introduction to Reinforcement Learning</h2>
        <h3>using the OpenAI platforms</h3>
<p><a target="_blank" href="TODO">
    TODO
</a></p>
<h4><a href="http://zeigermann.eu">Oliver Zeigermann</a> / <a href="http://twitter.com/djcordhose">@DJCordhose</a>
</h4>
<p><small><a href="TODO">
    TODO
</a></small></p>
</section>

<section data-markdown style="font-size: xx-large">
    <textarea data-template>
### Text Books

* Standard: Reinforcement Learning: An Introduction
  * Main page (including Code): http://incompleteideas.net/book/the-book-2nd.html
  * Free PDF: https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view

* More concise, more mathematical: Algorithms for Reinforcement Learning
  * Main page: https://sites.ualberta.ca/~szepesva/RLBook.html
  * Free PDF: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf
</textarea>
</section>


<section data-markdown>
    <textarea data-template>
## Types of Learning
<img src='img/types-of-ml.jpg'>
<small>
https://www.facebook.com/nipsfoundation/posts/795861577420073/
<br>
https://ranzato.github.io/publications/tutorial_deep_unsup_learning_part1_NeurIPS2018.pdf
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Learning to Play Chess using Reinforcement Learning
<a href='https://frpays.github.io/lc0-js/engine.html'>
<img src='img/lczero.jpg' height="400px">
</a>
<small>
https://frpays.github.io/lc0-js/engine.html
<br>
https://twitter.com/frpays/status/1077672273673359361
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Idea of Reinforcement Learning

control a system so as to maximize some numerical value which represents a long-term objective

<img src='img/rl_szepesva.png' height="350px">

<small>
https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf
</small>
</textarea>
</section>


<section data-markdown>
<textarea data-template>
### When to do Reinforcement Learning

1. When you don't have suitable data
1. You can accurately simulate an environment
1. Maybe express your problem as a game

<!-- <small>
TensorFlow based tool: https://github.com/deepmind/trfl/blob/master/docs/index.md
</small> -->
</textarea>
</section>


<section data-markdown>
<textarea data-template>
### Reinforcement Learning

<div  style="max-width: 45%; float: left">
<br>
<img src="img/rl.png">
</div>
<div style="max-width: 50%; float: right">
<br>
<ol>
    <li>Based on _Observations_ an _Agent_ executes </li>
<li>_Actions_ within a given  
<li>_Environment_ which lead to positive or negative 
<li>_Rewards_.
</ol>
</div>

<div style="clear: both;">
    <br>
    <p >The Agent’s job is to maximise the cumulative Reward</p>        
</div>

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Reward Hypothesis

_All_ goals can be described by the maximisation of expected cumulative reward

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### OpenAI Platforms

<img src='img/openai.jpg' height="500px">

<small>
https://openai.com/systems/
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Atari Reinforcement Learning

<img src='img/atari_rl.jpg'>
<small>
http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf    
</small>
</textarea>
</section>

<section data-markdown style="font-size: xx-large">
<textarea data-template>
### Hands-On Colab: Atari Environment using OpenAI

<a href='https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/openai-colab.ipynb'>
<img src='img/space-invaders-gym.png'>
</a>

* Score given as Reward
* Observation Space: Pixels in three color channels
* Action Space: 8 directions, plus fire on Atari joystick

<small>
https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/openai-colab.ipynb
<br>
https://github.com/openai/gym/blob/master/gym/envs/atari/atari_env.py
<br>
https://github.com/openai/atari-py
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Our Example: Consultant Travel

* Visit each customer and return home
* Reward per customer visited 1000
* Each travel has a penalty up to 1000
* Three customers, thus theoretical optimum 3000 (given 0 penalty)

</textarea>
</section>

<section data-markdown>
<textarea data-template>
### Reinforcement Learning Challenges

1. Model your problem into an environment 
  1. What is the observation space?
  1. What is the action space?
  1. What is the current situation?
  1. How rewarding is the current situation?
1. Give your agent a learning policy powerful enough to master your problem

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Step 1: Model environment

</textarea>
</section>


<section data-markdown>
    <textarea data-template>
### Consultant Travel: Graph

<img src='img/rl/berater/routes.jpg' height="500">

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Consultant Travel: Cost

<img src='img/rl/berater/routes-cost.jpg' height="500">

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Consultant Travel: Actions

<img src='img/rl/berater/routes-actions.jpg' height="500">

</textarea>
</section>
<section>
<h3>Perfect Path and Reward</h3>

<pre><code contenteditable data-trim class="line-numbers python">
S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000
A --0-> B R= 0.25 totalR= 0.55 cost= 250 customerR=1000
B --1-> C R= 0.25 totalR= 0.80 cost= 250 customerR=1000
C --2-> S R=-0.07 totalR= 0.73 cost= 200 customerR=   0
                </code></pre>

<img src='img/rl/berater/routes-actions.jpg' height="350">

<p>Reward: 1000, penalty: 800, normalized: 0.73</p>
        </section>
            

<section data-markdown>
    <textarea data-template>
### Prerequisite: Markovian Decision Processes

_Model your observation in a way that how you came to a certain state does not matter any more_

E.g. the position of the PacMan is not enough. Which path taken results in where the ghosts are and what pills have been eaten
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Modelling Observation

agent needs an idea of how the world looks like

* position of agent is just one information
* but not enough, as it not self-sufficient according to Markovian Decision Processes
* we also need to know that rewards go away once a customer has been visited
* there are several ways of modelling this
  * fully observed environment (e.g. Jump'n'Run Game)
  * partially observed environment (e.g. Ego Shooter)

<small>
https://spinningup.openai.com/en/latest/spinningup/rl_intro.html    
</small>
        
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Our Observation Model

calculate all combined rewards for all routes

<pre><code contenteditable data-trim class="line-numbers python">
# Initial position and rewards
[  0. 900. 600. 800. 750. 600. 750.]    

# Final position and rewards
[   0. -100. -400. -200. -250. -400. -250.]

</code></pre>

very friendly for learning

_Alternative: only show rewards for the routes robot can take from current position_


</textarea>
</section>

<section data-markdown>
    <textarea data-template>
## Step 2: Policy

_select an action based on the observed state of our consultant environment_

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Policy Gradient for Discrete Actions

use a neural network to approximate action probabilities

<img src='img/hidber-policy-gradient.jpg'>
<small>
Image Courtesy of Christian Hidber
<br>
https://spinningup.openai.com/en/latest/algorithms/vpg.html
</small>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Algorithm

* Policy: probabilities for which action is best in which situation
* Sampling Strategy: what experiments to make, what data to create
  * make experiments according to probabilities for actions 
</textarea>
</section>

<section data-markdown style="font-size: xx-large">
    <textarea data-template>
### Algorithms: More general

* RL as a learning problem best described using Markovian Decision Processes (MDPs)
* MDPs describe states and probabilities for going from one sate to another
* standard approach to ‘solve’ MDPs is to use dynamic programming
* dynamic programming transforms the problem of finding a good controller into the problem of finding a good value function
* apart from the simplest cases dynamic programming is infeasible
* turning the infeasible dynamic programming methods into practical algorithms
  * use powerful function approximation methods to compactly represent value functions
  * allows dealing with large, high-dimensional state- and action-spaces
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Algorithm Overview

<img src='img/rl_algorithms_9_15.svg'>

<small>
https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html    
</small>

</textarea>
</section>

<section data-markdown>
        <textarea data-template>
### Hands-On: Tweak Learning

Balance
1. Exploitation vs Exploration
1. Immediate vs Future Rewards (set Gamma between 0.9 and 0.99)

<img src='img/cartpole-animation.gif' height="200px">


<small>
https://en.wikipedia.org/wiki/Q-learning#Influence_of_variables
<br>
https://storage.googleapis.com/tfjs-examples/cart-pole/dist/index.html    
</small>
            </textarea>
        </section>
            
<section data-markdown class="todo">
    <textarea data-template>

# FROM HERE JUST MATERIAL

</textarea>
</section>


<section data-markdown class="todo">
    <textarea data-template>
### Advanced Deep Learning & Reinforcement Learning

https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs

</textarea>
</section>

<section data-markdown class="todo">
    <textarea data-template>
### Intro Course

https://www.youtube.com/watch?v=2pWv7GOvuf0

http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html

http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf

</textarea>
</section>

<section data-markdown class="todo">
        <textarea data-template>
### More intro

https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa

</textarea>
</section>
<section data-markdown class="todo">
        <textarea data-template>
### Installation for RL Notebook without restart
            
http://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/
</textarea>
</section>

<section data-markdown class="todo">
    <textarea data-template>
1. Notebook
   1. Step 2b: Text based representations
      1. Choose Example
      1. Write simple Code by hand that plays the game
   1. Step 3: Baselines
1. Folien übersetzen
   </textarea>
</section>

<section data-markdown class="todo">
    <textarea data-template>
### Structure

1. Technical Short Intro: RL with Open AI
  * https://towardsdatascience.com/reinforcement-learning-with-openai-d445c2c687d2
  * https://twitter.com/DeepMindAI/status/1065930524634558466
  * https://towardsdatascience.com/reinforcement-learning-from-scratch-designing-and-solving-a-task-all-within-a-python-notebook-48c40021da4
1. What is RL
1. When to apply
1. Problem without Labels: Route Planing?
1. How to model a problem for rl?
1. Making it run on Colab with OpenAI Gym
1. How to make it more general?
1. Evaluation

</textarea>
</section>




<section data-markdown>
<textarea data-template>
<img src='img/robot-fun.jpg' height="500px">

<p class="fragment">Reinforcement Learning: am ehesten die Basis für Roboter, die uns Menschen ausrotten</p>
</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Aber erstmal machen wir die Roboter fertig!
<video controls src="video/knocked-over-stand-up.mp4"  muted type="video/mp4" height="500"></video>
            
<small>
https://blog.openai.com/openai-baselines-ppo/
</small>
</textarea>
</section>
<section data-markdown>
<textarea data-template>
### Und dann müssen die vorher auch noch ihren Pulli ankriegen
<a href='https://youtu.be/ixmE5nt2o88?t=49'>
<img src='img/rl-pulli.png'>
</a>

<small>
https://twitter.com/evolvingstuff/status/1058220030045827072    
</small>
</textarea>
</section>

<section data-markdown class="todo">
    <textarea data-template>
### Example from Supervised RNN to RL

RL Addition Simulator für RNN -> RL
</textarea>
</section>

<section data-markdown class="todo">
    <textarea data-template>
### Colab Notebooks

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v2.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v3.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v4.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v5.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/colab-sandbox.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/openai-gym-playground.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/openai-colab.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/baselines-colab.ipynb

https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/dqn.ipynb

</textarea>
</section>

<section data-markdown>
    <textarea data-template>
### Dopamine

* alternative for trying out new strategies
* Research centered
* restricted to Atair games
* so not so much for building your own environments

<small>
http://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html
</small>
</textarea>
</section>

<section data-markdown class="todo">
    <textarea data-template>
### Overfitting


* Das wird irgendwie nicht so breit diskutiert. Die meisten arbeiten ja mit Games, da ist es vermutlich auch nicht so ein Thema, da dort natürlicherweise a) mit einem random start state begonnen wird und b) das environment selbst oft randomness verwendet um den nächsten state zu definieren. Bei problemen aus «dem echten leben» dürfte das nicht immer zutreffen.
* https://medium.com/singular-distillation/games-games-games-fighting-overfitting-in-reinforcement-learning-b464ed20cb1d
* hier noch ein paper (habe ich nur überflogen): https://arxiv.org/abs/1804.06893
* https://blog.openai.com/quantifying-generalization-in-reinforcement-learning/

</textarea>
</section>

<section>
    <h2>Wrap Up</h2>
    <ul>
        <li class="fragment"> 

    </ul>
    <p>
            <em>Introduction to Reinforcement Learning</em>
        <br>
        <br>
        <small>
    <a href="http://zeigermann.eu">Oliver Zeigermann</a> / <a href="http://twitter.com/djcordhose">@DJCordhose</a>
        <br>
<a href="TODO">
    TODO</a>
</small>
    </p>
</section>
        

    </div>

</div>

<script src="reveal.js/lib/js/head.min.js"></script>
<script src="reveal.js/js/reveal.js"></script>
<script src="lib/jquery-2.2.4.js"></script>

<script>
        $('.slido').remove();
        if (window.location.hostname.indexOf('localhost') !== -1) {
            // only applies to local version
            $('.remote').remove();
        } else {
            // only applies to public version
            $('.todo').remove();
            $('.preparation').remove();
            $('.local').remove();
        }
        Reveal.addEventListener( 'ready', function( event ) {
                // do we want this???
            $('li').addClass('fragment')

            if (window.location.hostname.indexOf('localhost') !== -1) {
                // only applies to presentation version
                Reveal.configure({ controls: false });
            } else {
                // only applies to public version
                $('.fragment').removeClass('fragment');
            }
            // applies to all versions
            $('code').addClass('line-numbers');

            // make all links open in new tab
            $('a').attr('target', '_blank')

        } );
        // $('section').attr('data-background-image', "backgrounds/light-metal.jpg");
        // $('section').attr('data-background-image', "backgrounds/pink.jpg");
        // $('section').attr('data-background-image', "backgrounds/white.jpg");
        $('section').attr('data-background-image', "backgrounds/sky.jpg");

    //    $('section').attr('data-background-image', "backgrounds/code.jpg");
    </script>
    
<script>

    // Full list of configuration options available at:
    // https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        controls: true,
        progress: true,
        history: true,
        center: true,
        width: 1100,


        transition: 'fade', // none/fade/slide/convex/concave/zoom

        math: {
            mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
            config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // Optional reveal.js plugins
        dependencies: [
            {
                src: 'reveal.js/lib/js/classList.js', condition: function () {
                return !document.body.classList;
            }
            },
            {
                src: 'reveal.js/plugin/markdown/marked.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/markdown/markdown.js', condition: function () {
                return !!document.querySelector('[data-markdown]');
            }
            },
            {
                src: 'reveal.js/plugin/highlight/highlight.js', async: true, condition: function () {
                return !!document.querySelector('pre code');
            }, callback: function () {
                hljs.initHighlightingOnLoad();
            }
            },
            {src: 'reveal.js/plugin/zoom-js/zoom.js', async: true},
            {src: 'reveal.js/plugin/notes/notes.js', async: true},
            // https://github.com/mikemiles86/reveal-line-numbers
            {src: 'lib/js/line-numbers.js'},
            { src: 'reveal.js/plugin/math/math.js', async: true }
        ]
    });

</script>

</body>
</html>
