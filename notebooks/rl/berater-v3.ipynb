{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "berater-v3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v3\n",
        "\n",
        "## Changes from v2\n",
        "1. changed learning strategy from ppo to dqn (seems more intuitive)\n",
        "1. Fixed bug in env#reset still returning the the scalar position instead of the complete state  \n",
        "\n",
        "## Next Steps\n",
        "1. choose costs of traversal randomly with each episode\n",
        "  * aim: agent will (hopefully) be able to work with any costs\n",
        "1. train a different graph with each episode \n",
        "  * aim: agent can work on any graph"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation (required for colab)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install git+https://github.com/openai/baselines >/dev/null\n",
        "# !pip install gym >/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7Y3XKzw1kQ3I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnt=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n",
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 3 discrete deterministic actions:\n",
        "    - 0: First Direction\n",
        "    - 1: Second Direction\n",
        "    - 2: Third Direction / Go home\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    num_envs = 1\n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    showRender = False\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "        self.map = {\n",
        "            'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "            'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "            'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "            'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "        }\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=numpy.array([0,-1000,-1000,-1000,-1000,-1000,-1000]),\n",
        "                                             high=numpy.array([3,1000,1000,1000,1000,1000,1000]),\n",
        "                                             dtype=numpy.float32)\n",
        "\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, actionArg):\n",
        "        paths = self.map[self.state]\n",
        "        action = actionArg\n",
        "        destination, cost = paths[action]\n",
        "        lastState = self.state\n",
        "        lastObState = state_name_to_int(lastState)\n",
        "        customerReward = self.customer_reward[destination]\n",
        "\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        self.state = destination\n",
        "        reward = (-cost + self.customer_reward[destination]) / self.optimum\n",
        "        self.customer_visited(destination)\n",
        "        done = destination == 'S' and self.all_customers_visited()\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   #lastState + ':' + str(lastObState) + ' --' + str(action) + '-> ' + self.state + ':' + str(stateAsInt) +\n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone or (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = numpy.array([ position, \n",
        "                               self.getEdgeObservation('S','A'),\n",
        "                               self.getEdgeObservation('S','B'),\n",
        "                               self.getEdgeObservation('S','C'),\n",
        "                               self.getEdgeObservation('A','B'),\n",
        "                               self.getEdgeObservation('A','C'),\n",
        "                               self.getEdgeObservation('B','C'),\n",
        "                              ],\n",
        "                             dtype=numpy.float32)\n",
        "        return result\n",
        "\n",
        "    def getEdgeObservation(self, source, target):\n",
        "        reward = self.customer_reward[target] \n",
        "        cost = self.getCost(source,target)\n",
        "        result = reward - cost\n",
        "\n",
        "        return result\n",
        "\n",
        "    def getCost(self, source, target):\n",
        "        paths = self.map[source]\n",
        "        targetIndex=state_name_to_int(target)\n",
        "        for destination, cost in paths:\n",
        "            if destination == target:\n",
        "                result = cost\n",
        "                break\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "    def reset(self):\n",
        "        # print(\"Reset\")\n",
        "        \n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "        reward_per_customer = 1000\n",
        "        self.customer_reward = {\n",
        "            'S': 0,\n",
        "            'A': reward_per_customer,\n",
        "            'B': reward_per_customer,\n",
        "            'C': reward_per_customer,\n",
        "        }\n",
        "\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if BeraterEnv.showRender:\n",
        "            print( (\"steps=%4.0f  \" % self.stepCount) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + ' done=' + str(self.isDone))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kWF5vSsakQ3b"
      },
      "cell_type": "markdown",
      "source": [
        "# Register Einvornment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SU1NxPMokQ3e",
        "outputId": "3b3a5e4f-1a48-4e1c-8925-88da80bc9934",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "cnt += 1\n",
        "id = \"Berater-v{}\".format(cnt)\n",
        "register(\n",
        "    id=id,\n",
        "    entry_point=BeraterEnv\n",
        ")   \n",
        "\n",
        "print(id)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater-v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Usj9iWTskQ3t"
      },
      "cell_type": "markdown",
      "source": [
        "# Try out Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oTtUfeONkQ3w",
        "outputId": "0759c2c2-e979-4721-dfbe-5ebdaedbb40b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = True\n",
        "\n",
        "env = gym.make(id)\n",
        "observation = env.reset()\n",
        "print(env)\n",
        "\n",
        "for t in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        env.render()\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BeraterEnv<Berater-v1>>\n",
            "Episode:    0   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    2  A --1-> C R= 0.20 totalR= 0.50 cost= 400 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    3  C --0-> A R=-0.13 totalR= 0.37 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    4  A --1-> C R=-0.13 totalR= 0.23 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    5  C --1-> B R= 0.25 totalR= 0.48 cost= 250 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    6  B --2-> S R=-0.13 totalR= 0.35 cost= 400 customerR=   0 optimum=3000\n",
            "Done: episodes=     1  avgSteps=  6.00  avgTotalReward= 0.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4GlYjZ3xkQ38"
      },
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NzbylmYAkQ3-",
        "outputId": "00f18d0a-1e91-489d-9598-bc535d1712aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2992
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from baselines import deepq\n",
        "\n",
        "BeraterEnv.showStep = False\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "env = gym.make(id)\n",
        "\n",
        "# https://en.wikipedia.org/wiki/Q-learning#Influence_of_variables\n",
        "model = deepq.learn(\\\n",
        "        env,\\\n",
        "        seed=42,\\\n",
        "        network='mlp',\\\n",
        "        lr=1e-3,\\\n",
        "        total_timesteps=30000,\\\n",
        "        buffer_size=50000,\\\n",
        "        exploration_fraction=0.5,\\\n",
        "        exploration_final_eps=0.02,\\\n",
        "        print_freq=1000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logging to /tmp/openai-2018-12-25-17-06-31-197203\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done: episodes=   100  avgSteps=  8.61  avgTotalReward= 0.23\n",
            "Done: episodes=   200  avgSteps=  8.29  avgTotalReward= 0.26\n",
            "Done: episodes=   300  avgSteps=  7.80  avgTotalReward= 0.30\n",
            "Done: episodes=   400  avgSteps=  7.77  avgTotalReward= 0.33\n",
            "Done: episodes=   500  avgSteps=  7.73  avgTotalReward= 0.30\n",
            "Done: episodes=   600  avgSteps=  7.17  avgTotalReward= 0.37\n",
            "Done: episodes=   700  avgSteps=  6.78  avgTotalReward= 0.41\n",
            "Done: episodes=   800  avgSteps=  7.04  avgTotalReward= 0.39\n",
            "Done: episodes=   900  avgSteps=  6.46  avgTotalReward= 0.43\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 71       |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 0.4      |\n",
            "| steps                   | 7387     |\n",
            "--------------------------------------\n",
            "Done: episodes=  1000  avgSteps=  6.32  avgTotalReward= 0.44\n",
            "Done: episodes=  1100  avgSteps=  6.12  avgTotalReward= 0.46\n",
            "Done: episodes=  1200  avgSteps=  6.23  avgTotalReward= 0.47\n",
            "Done: episodes=  1300  avgSteps=  5.88  avgTotalReward= 0.49\n",
            "Done: episodes=  1400  avgSteps=  6.09  avgTotalReward= 0.47\n",
            "Saving model due to mean reward increase: None -> 0.5\n",
            "Done: episodes=  1500  avgSteps=  5.76  avgTotalReward= 0.51\n",
            "Done: episodes=  1600  avgSteps=  5.64  avgTotalReward= 0.52\n",
            "Done: episodes=  1700  avgSteps=  5.70  avgTotalReward= 0.51\n",
            "Done: episodes=  1800  avgSteps=  5.85  avgTotalReward= 0.50\n",
            "Done: episodes=  1900  avgSteps=  5.40  avgTotalReward= 0.53\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 48       |\n",
            "| episodes                | 2000     |\n",
            "| mean 100 episode reward | 0.6      |\n",
            "| steps                   | 13180    |\n",
            "--------------------------------------\n",
            "Done: episodes=  2000  avgSteps=  5.22  avgTotalReward= 0.57\n",
            "Done: episodes=  2100  avgSteps=  5.40  avgTotalReward= 0.54\n",
            "Done: episodes=  2200  avgSteps=  5.11  avgTotalReward= 0.57\n",
            "Done: episodes=  2300  avgSteps=  5.09  avgTotalReward= 0.59\n",
            "Done: episodes=  2400  avgSteps=  4.92  avgTotalReward= 0.59\n",
            "Done: episodes=  2500  avgSteps=  5.22  avgTotalReward= 0.57\n",
            "Done: episodes=  2600  avgSteps=  4.95  avgTotalReward= 0.60\n",
            "Done: episodes=  2700  avgSteps=  4.85  avgTotalReward= 0.61\n",
            "Done: episodes=  2800  avgSteps=  4.92  avgTotalReward= 0.61\n",
            "Done: episodes=  2900  avgSteps=  4.82  avgTotalReward= 0.61\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 28       |\n",
            "| episodes                | 3000     |\n",
            "| mean 100 episode reward | 0.6      |\n",
            "| steps                   | 18180    |\n",
            "--------------------------------------\n",
            "Done: episodes=  3000  avgSteps=  4.71  avgTotalReward= 0.62\n",
            "Done: episodes=  3100  avgSteps=  4.65  avgTotalReward= 0.64\n",
            "Done: episodes=  3200  avgSteps=  4.84  avgTotalReward= 0.62\n",
            "Done: episodes=  3300  avgSteps=  4.59  avgTotalReward= 0.64\n",
            "Saving model due to mean reward increase: 0.5 -> 0.7\n",
            "Done: episodes=  3400  avgSteps=  4.58  avgTotalReward= 0.66\n",
            "Done: episodes=  3500  avgSteps=  4.40  avgTotalReward= 0.68\n",
            "Done: episodes=  3600  avgSteps=  4.64  avgTotalReward= 0.64\n",
            "Done: episodes=  3700  avgSteps=  4.41  avgTotalReward= 0.67\n",
            "Done: episodes=  3800  avgSteps=  4.40  avgTotalReward= 0.67\n",
            "Done: episodes=  3900  avgSteps=  4.33  avgTotalReward= 0.68\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 11       |\n",
            "| episodes                | 4000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 22701    |\n",
            "--------------------------------------\n",
            "Done: episodes=  4000  avgSteps=  4.37  avgTotalReward= 0.69\n",
            "Done: episodes=  4100  avgSteps=  4.24  avgTotalReward= 0.71\n",
            "Done: episodes=  4200  avgSteps=  4.21  avgTotalReward= 0.70\n",
            "Done: episodes=  4300  avgSteps=  4.10  avgTotalReward= 0.72\n",
            "Done: episodes=  4400  avgSteps=  4.11  avgTotalReward= 0.71\n",
            "Done: episodes=  4500  avgSteps=  4.11  avgTotalReward= 0.72\n",
            "Done: episodes=  4600  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  4700  avgSteps=  4.05  avgTotalReward= 0.72\n",
            "Done: episodes=  4800  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  4900  avgSteps=  4.10  avgTotalReward= 0.72\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 26806    |\n",
            "--------------------------------------\n",
            "Done: episodes=  5000  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  5100  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  5200  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  5300  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  5400  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  5500  avgSteps=  4.06  avgTotalReward= 0.73\n",
            "Done: episodes=  5600  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  5700  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  5800  avgSteps=  4.08  avgTotalReward= 0.72\n",
            "Done: episodes=  5900  avgSteps=  4.06  avgTotalReward= 0.72\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 30847    |\n",
            "--------------------------------------\n",
            "Done: episodes=  6000  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  6100  avgSteps=  4.13  avgTotalReward= 0.72\n",
            "Done: episodes=  6200  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  6300  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  6400  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  6500  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  6600  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  6700  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  6800  avgSteps=  4.06  avgTotalReward= 0.72\n",
            "Done: episodes=  6900  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 7000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 34903    |\n",
            "--------------------------------------\n",
            "Done: episodes=  7000  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  7100  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  7200  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  7300  avgSteps=  4.06  avgTotalReward= 0.73\n",
            "Done: episodes=  7400  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  7500  avgSteps=  4.05  avgTotalReward= 0.72\n",
            "Done: episodes=  7600  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  7700  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  7800  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes=  7900  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 8000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 38937    |\n",
            "--------------------------------------\n",
            "Done: episodes=  8000  avgSteps=  4.00  avgTotalReward= 0.73\n",
            "Done: episodes=  8100  avgSteps=  4.10  avgTotalReward= 0.72\n",
            "Done: episodes=  8200  avgSteps=  4.08  avgTotalReward= 0.72\n",
            "Done: episodes=  8300  avgSteps=  4.06  avgTotalReward= 0.73\n",
            "Done: episodes=  8400  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  8500  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes=  8600  avgSteps=  4.12  avgTotalReward= 0.72\n",
            "Done: episodes=  8700  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  8800  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  8900  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 9000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 42997    |\n",
            "--------------------------------------\n",
            "Done: episodes=  9000  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  9100  avgSteps=  4.08  avgTotalReward= 0.73\n",
            "Done: episodes=  9200  avgSteps=  4.06  avgTotalReward= 0.73\n",
            "Done: episodes=  9300  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  9400  avgSteps=  4.10  avgTotalReward= 0.72\n",
            "Done: episodes=  9500  avgSteps=  4.02  avgTotalReward= 0.73\n",
            "Done: episodes=  9600  avgSteps=  4.05  avgTotalReward= 0.73\n",
            "Done: episodes=  9700  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "Done: episodes=  9800  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes=  9900  avgSteps=  4.04  avgTotalReward= 0.73\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10000    |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 47040    |\n",
            "--------------------------------------\n",
            "Done: episodes= 10000  avgSteps=  4.01  avgTotalReward= 0.73\n",
            "Done: episodes= 10100  avgSteps=  4.03  avgTotalReward= 0.72\n",
            "Done: episodes= 10200  avgSteps=  4.03  avgTotalReward= 0.73\n",
            "Done: episodes= 10300  avgSteps=  4.03  avgTotalReward= 0.72\n",
            "Done: episodes= 10400  avgSteps=  4.06  avgTotalReward= 0.73\n",
            "Done: episodes= 10500  avgSteps=  4.07  avgTotalReward= 0.72\n",
            "Done: episodes= 10600  avgSteps=  4.10  avgTotalReward= 0.72\n",
            "Done: episodes= 10700  avgSteps=  4.07  avgTotalReward= 0.73\n",
            "Restored model with mean reward: 0.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TtBh4c6-kQ4K"
      },
      "cell_type": "markdown",
      "source": [
        "# Enjoy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ucP0gNhhkQ4O",
        "outputId": "d83d62ea-90ac-4046-a9da-392755f3566f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "observation = env.reset()\n",
        "state = np.zeros((1, 2*128))\n",
        "dones = np.zeros((1))\n",
        "\n",
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "for t in range(1000):\n",
        "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "    observation, reward, done, info = env.step(actions[0])\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 10728   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
            "Episode: 10728   Step:    2  A --0-> B R= 0.25 totalR= 0.55 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 10728   Step:    3  B --1-> C R= 0.25 totalR= 0.80 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 10728   Step:    4  C --2-> S R=-0.07 totalR= 0.73 cost= 200 customerR=   0 optimum=3000\n",
            "Episode finished after 4 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5fY1da_0l15E",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}