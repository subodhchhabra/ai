{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "berater-v11.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v11\n",
        "\n",
        "## Changes from v10\n",
        "* configure custom network \n",
        "  * including L2 regularization / Dropout\n",
        "  * not possible to just configure these two\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation (required for colab)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/baselines >/dev/null\n",
        "!pip install gym >/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3OdHyWEEEwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n",
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 4 discrete deterministic actions, each choosing one direction\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "#         self.map = {\n",
        "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "#         }\n",
        "        self.map = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "        }\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "      \n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                             high=high,\n",
        "                                             dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "          return paths[action]\n",
        "        else:\n",
        "          # sorry, no such action, stay where you are and pay a high penalty\n",
        "          return (state, 1000)\n",
        "      \n",
        "    def step(self, action):\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "        lastState = self.state\n",
        "        customerReward = self.customer_reward[destination]\n",
        "        reward = (customerReward - cost) / self.optimum\n",
        "\n",
        "        self.state = destination\n",
        "        self.customer_visited(destination)\n",
        "        done = destination == 'S' and self.all_customers_visited()\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([ position, \n",
        "                               self.getPathObservation(position, 0),\n",
        "                               self.getPathObservation(position, 1),\n",
        "                               self.getPathObservation(position, 2),\n",
        "                               self.getPathObservation(position, 3)\n",
        "                              ],\n",
        "                             dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        source = int_to_state_name(position)\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "          target, cost = paths[path]\n",
        "          reward = self.customer_reward[target] \n",
        "          result = reward - cost\n",
        "        else:\n",
        "          result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "      \n",
        "    def modulate_reward(self):\n",
        "      number_of_customers = len(self.map) - 1\n",
        "      number_per_consultant = int(number_of_customers/2)\n",
        "#       number_per_consultant = int(number_of_customers/1.5)\n",
        "      self.customer_reward = {\n",
        "          'S': 0\n",
        "      }\n",
        "      for customer_nr in range(1, number_of_customers + 1):\n",
        "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
        "      \n",
        "      # every consultant only visits a few random customers\n",
        "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "      key_list = list(self.customer_reward.keys())\n",
        "      for sample in samples:\n",
        "        self.customer_reward[key_list[sample]] = 1000\n",
        "\n",
        "      \n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.modulate_reward()\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "      \n",
        "    def render(self):\n",
        "      print(self.customer_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wdZBH30Rs95B",
        "colab_type": "code",
        "outputId": "89533d3c-6976-4041-b7e3-64ff743ad29f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "env = BeraterEnv()\n",
        "print(env.reset())\n",
        "print(env.customer_reward)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    0.  -300.  -100.   800. -1000.     0.     0.     0.  1000.  1000.\n",
            "     0.     0.  1000.  1000.     0.     0.  1000.     0.  1000.]\n",
            "{'S': 0, 'A': 0, 'B': 0, 'C': 1000, 'D': 1000, 'E': 0, 'F': 0, 'G': 1000, 'H': 1000, 'K': 0, 'L': 0, 'M': 1000, 'N': 0, 'O': 1000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Usj9iWTskQ3t"
      },
      "cell_type": "markdown",
      "source": [
        "# Try out Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oTtUfeONkQ3w",
        "outputId": "c740afa6-9688-4fdf-937d-0229b51893d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3540
        }
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = True\n",
        "\n",
        "env = BeraterEnv()\n",
        "print(env)\n",
        "observation = env.reset()\n",
        "print(observation)\n",
        "\n",
        "for t in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()\n",
        "print(observation)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BeraterEnv instance>\n",
            "[    0.   700.   900.  -200. -1000.     0.  1000.  1000.     0.  1000.\n",
            "     0.     0.     0.  1000.  1000.     0.     0.  1000.     0.]\n",
            "Episode:    0   Step:    1  S --0-> A R= 0.12 totalR= 0.12 cost= 300 customerR=1000 optimum=6000\n",
            "Episode:    0   Step:    2  A --3-> D R= 0.15 totalR= 0.27 cost= 100 customerR=1000 optimum=6000\n",
            "Episode:    0   Step:    3  D --1-> F R=-0.01 totalR= 0.26 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:    4  F --0-> D R=-0.01 totalR= 0.25 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:    5  D --3-> D R=-0.17 totalR= 0.08 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:    6  D --3-> D R=-0.17 totalR=-0.08 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:    7  D --3-> D R=-0.17 totalR=-0.25 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:    8  D --3-> D R=-0.17 totalR=-0.42 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:    9  D --1-> F R=-0.01 totalR=-0.42 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   10  F --3-> F R=-0.17 totalR=-0.59 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   11  F --1-> E R=-0.02 totalR=-0.61 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   12  E --2-> H R= 0.15 totalR=-0.46 cost= 100 customerR=1000 optimum=6000\n",
            "Episode:    0   Step:   13  H --0-> E R=-0.02 totalR=-0.48 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   14  E --3-> E R=-0.17 totalR=-0.64 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   15  E --2-> H R=-0.02 totalR=-0.66 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   16  H --0-> E R=-0.02 totalR=-0.68 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   17  E --0-> A R=-0.02 totalR=-0.69 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   18  A --0-> S R=-0.05 totalR=-0.74 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   19  S --2-> C R=-0.03 totalR=-0.78 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   20  C --1-> B R= 0.16 totalR=-0.62 cost=  50 customerR=1000 optimum=6000\n",
            "Episode:    0   Step:   21  B --2-> C R=-0.01 totalR=-0.63 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   22  C --3-> L R=-0.03 totalR=-0.66 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   23  L --3-> L R=-0.17 totalR=-0.83 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   24  L --2-> L R=-0.17 totalR=-0.99 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   25  L --0-> C R=-0.03 totalR=-1.03 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   26  C --1-> B R=-0.01 totalR=-1.03 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   27  B --1-> A R=-0.02 totalR=-1.05 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   28  A --1-> B R=-0.02 totalR=-1.07 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   29  B --1-> A R=-0.02 totalR=-1.08 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   30  A --0-> S R=-0.05 totalR=-1.13 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   31  S --1-> B R=-0.02 totalR=-1.15 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   32  B --0-> S R=-0.02 totalR=-1.17 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   33  S --3-> S R=-0.17 totalR=-1.33 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   34  S --0-> A R=-0.05 totalR=-1.38 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   35  A --3-> D R=-0.02 totalR=-1.40 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   36  D --1-> F R=-0.01 totalR=-1.41 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   37  F --2-> G R=-0.03 totalR=-1.44 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   38  G --3-> G R=-0.17 totalR=-1.61 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   39  G --3-> G R=-0.17 totalR=-1.78 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   40  G --0-> F R=-0.03 totalR=-1.81 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   41  F --2-> G R=-0.03 totalR=-1.84 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   42  G --3-> G R=-0.17 totalR=-2.01 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   43  G --0-> F R=-0.03 totalR=-2.04 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   44  F --1-> E R=-0.02 totalR=-2.06 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   45  E --3-> E R=-0.17 totalR=-2.23 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   46  E --1-> F R=-0.02 totalR=-2.24 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   47  F --3-> F R=-0.17 totalR=-2.41 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   48  F --3-> F R=-0.17 totalR=-2.57 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   49  F --2-> G R=-0.03 totalR=-2.61 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   50  G --3-> G R=-0.17 totalR=-2.77 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   51  G --0-> F R=-0.03 totalR=-2.81 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   52  F --1-> E R=-0.02 totalR=-2.82 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   53  E --1-> F R=-0.02 totalR=-2.84 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   54  F --1-> E R=-0.02 totalR=-2.86 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   55  E --3-> E R=-0.17 totalR=-3.02 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   56  E --0-> A R=-0.02 totalR=-3.04 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   57  A --3-> D R=-0.02 totalR=-3.06 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   58  D --2-> D R=-0.17 totalR=-3.22 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   59  D --0-> A R=-0.02 totalR=-3.24 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   60  A --3-> D R=-0.02 totalR=-3.26 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   61  D --3-> D R=-0.17 totalR=-3.42 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   62  D --2-> D R=-0.17 totalR=-3.59 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   63  D --3-> D R=-0.17 totalR=-3.76 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   64  D --2-> D R=-0.17 totalR=-3.92 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   65  D --3-> D R=-0.17 totalR=-4.09 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   66  D --0-> A R=-0.02 totalR=-4.11 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   67  A --2-> E R=-0.02 totalR=-4.12 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   68  E --0-> A R=-0.02 totalR=-4.14 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   69  A --0-> S R=-0.05 totalR=-4.19 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   70  S --0-> A R=-0.05 totalR=-4.24 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   71  A --1-> B R=-0.02 totalR=-4.26 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   72  B --1-> A R=-0.02 totalR=-4.27 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   73  A --2-> E R=-0.02 totalR=-4.29 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   74  E --0-> A R=-0.02 totalR=-4.31 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   75  A --0-> S R=-0.05 totalR=-4.36 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   76  S --1-> B R=-0.02 totalR=-4.37 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   77  B --3-> K R= 0.13 totalR=-4.24 cost= 200 customerR=1000 optimum=6000\n",
            "Episode:    0   Step:   78  K --0-> B R=-0.03 totalR=-4.27 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   79  B --1-> A R=-0.02 totalR=-4.29 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   80  A --2-> E R=-0.02 totalR=-4.31 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   81  E --2-> H R=-0.02 totalR=-4.32 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   82  H --3-> H R=-0.17 totalR=-4.49 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   83  H --0-> E R=-0.02 totalR=-4.51 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   84  E --1-> F R=-0.02 totalR=-4.52 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   85  F --1-> E R=-0.02 totalR=-4.54 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   86  E --3-> E R=-0.17 totalR=-4.71 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   87  E --1-> F R=-0.02 totalR=-4.72 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   88  F --1-> E R=-0.02 totalR=-4.74 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   89  E --3-> E R=-0.17 totalR=-4.91 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   90  E --2-> H R=-0.02 totalR=-4.92 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   91  H --3-> H R=-0.17 totalR=-5.09 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   92  H --3-> H R=-0.17 totalR=-5.26 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   93  H --2-> H R=-0.17 totalR=-5.42 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   94  H --2-> H R=-0.17 totalR=-5.59 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   95  H --3-> H R=-0.17 totalR=-5.76 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   96  H --0-> E R=-0.02 totalR=-5.77 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   97  E --2-> H R=-0.02 totalR=-5.79 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   98  H --3-> H R=-0.17 totalR=-5.96 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:   99  H --1-> K R=-0.05 totalR=-6.01 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  100  K --0-> B R=-0.03 totalR=-6.04 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  101  B --1-> A R=-0.02 totalR=-6.06 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  102  A --2-> E R=-0.02 totalR=-6.07 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  103  E --0-> A R=-0.02 totalR=-6.09 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  104  A --3-> D R=-0.02 totalR=-6.11 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  105  D --0-> A R=-0.02 totalR=-6.12 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  106  A --2-> E R=-0.02 totalR=-6.14 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  107  E --0-> A R=-0.02 totalR=-6.16 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  108  A --3-> D R=-0.02 totalR=-6.17 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  109  D --3-> D R=-0.17 totalR=-6.34 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  110  D --0-> A R=-0.02 totalR=-6.36 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  111  A --3-> D R=-0.02 totalR=-6.37 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  112  D --0-> A R=-0.02 totalR=-6.39 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  113  A --0-> S R=-0.05 totalR=-6.44 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  114  S --0-> A R=-0.05 totalR=-6.49 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  115  A --0-> S R=-0.05 totalR=-6.54 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  116  S --2-> C R=-0.03 totalR=-6.57 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  117  C --3-> L R=-0.03 totalR=-6.61 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  118  L --0-> C R=-0.03 totalR=-6.64 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  119  C --3-> L R=-0.03 totalR=-6.67 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  120  L --2-> L R=-0.17 totalR=-6.84 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  121  L --3-> L R=-0.17 totalR=-7.01 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  122  L --3-> L R=-0.17 totalR=-7.17 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  123  L --1-> M R=-0.01 totalR=-7.18 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  124  M --1-> L R=-0.01 totalR=-7.19 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  125  L --1-> M R=-0.01 totalR=-7.20 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  126  M --0-> C R=-0.02 totalR=-7.22 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  127  C --1-> B R=-0.01 totalR=-7.22 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  128  B --1-> A R=-0.02 totalR=-7.24 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  129  A --1-> B R=-0.02 totalR=-7.26 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  130  B --3-> K R=-0.03 totalR=-7.29 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  131  K --0-> B R=-0.03 totalR=-7.32 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  132  B --3-> K R=-0.03 totalR=-7.36 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  133  K --1-> H R=-0.05 totalR=-7.41 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  134  H --2-> H R=-0.17 totalR=-7.57 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  135  H --0-> E R=-0.02 totalR=-7.59 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  136  E --1-> F R=-0.02 totalR=-7.61 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  137  F --2-> G R=-0.03 totalR=-7.64 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  138  G --0-> F R=-0.03 totalR=-7.67 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  139  F --2-> G R=-0.03 totalR=-7.71 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  140  G --0-> F R=-0.03 totalR=-7.74 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  141  F --1-> E R=-0.02 totalR=-7.76 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  142  E --3-> E R=-0.17 totalR=-7.92 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  143  E --2-> H R=-0.02 totalR=-7.94 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  144  H --2-> H R=-0.17 totalR=-8.11 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  145  H --1-> K R=-0.05 totalR=-8.16 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  146  K --0-> B R=-0.03 totalR=-8.19 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  147  B --3-> K R=-0.03 totalR=-8.22 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  148  K --1-> H R=-0.05 totalR=-8.28 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  149  H --1-> K R=-0.05 totalR=-8.33 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  150  K --3-> K R=-0.17 totalR=-8.49 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  151  K --0-> B R=-0.03 totalR=-8.53 cost= 200 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  152  B --2-> C R=-0.01 totalR=-8.53 cost=  50 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  153  C --2-> M R=-0.02 totalR=-8.55 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  154  M --3-> M R=-0.17 totalR=-8.72 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  155  M --2-> N R= 0.15 totalR=-8.57 cost= 100 customerR=1000 optimum=6000\n",
            "Episode:    0   Step:  156  N --3-> N R=-0.17 totalR=-8.73 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  157  N --3-> N R=-0.17 totalR=-8.90 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  158  N --3-> N R=-0.17 totalR=-9.07 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  159  N --2-> N R=-0.17 totalR=-9.23 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  160  N --1-> O R=-0.02 totalR=-9.25 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  161  O --2-> O R=-0.17 totalR=-9.42 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  162  O --2-> O R=-0.17 totalR=-9.58 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  163  O --3-> O R=-0.17 totalR=-9.75 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  164  O --2-> O R=-0.17 totalR=-9.92 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  165  O --3-> O R=-0.17 totalR=-10.08 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  166  O --3-> O R=-0.17 totalR=-10.25 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  167  O --2-> O R=-0.17 totalR=-10.42 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  168  O --2-> O R=-0.17 totalR=-10.58 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  169  O --3-> O R=-0.17 totalR=-10.75 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  170  O --0-> N R=-0.02 totalR=-10.77 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  171  N --1-> O R=-0.02 totalR=-10.78 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  172  O --2-> O R=-0.17 totalR=-10.95 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  173  O --3-> O R=-0.17 totalR=-11.12 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  174  O --2-> O R=-0.17 totalR=-11.28 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  175  O --1-> G R=-0.05 totalR=-11.33 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  176  G --2-> G R=-0.17 totalR=-11.50 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  177  G --1-> O R=-0.05 totalR=-11.55 cost= 300 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  178  O --0-> N R=-0.02 totalR=-11.57 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  179  N --2-> N R=-0.17 totalR=-11.73 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  180  N --2-> N R=-0.17 totalR=-11.90 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  181  N --3-> N R=-0.17 totalR=-12.07 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  182  N --0-> M R=-0.02 totalR=-12.08 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  183  M --3-> M R=-0.17 totalR=-12.25 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  184  M --2-> N R=-0.02 totalR=-12.27 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  185  N --3-> N R=-0.17 totalR=-12.43 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  186  N --0-> M R=-0.02 totalR=-12.45 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  187  M --0-> C R=-0.02 totalR=-12.47 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  188  C --2-> M R=-0.02 totalR=-12.48 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  189  M --0-> C R=-0.02 totalR=-12.50 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  190  C --2-> M R=-0.02 totalR=-12.52 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  191  M --3-> M R=-0.17 totalR=-12.68 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  192  M --2-> N R=-0.02 totalR=-12.70 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  193  N --2-> N R=-0.17 totalR=-12.87 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  194  N --3-> N R=-0.17 totalR=-13.03 cost=1000 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  195  N --0-> M R=-0.02 totalR=-13.05 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  196  M --0-> C R=-0.02 totalR=-13.07 cost= 100 customerR=   0 optimum=6000\n",
            "Episode:    0   Step:  197  C --0-> S R=-0.03 totalR=-13.10 cost= 200 customerR=   0 optimum=6000\n",
            "Done: episodes=     1  avgSteps=197.00  avgTotalReward=-13.10\n",
            "Episode finished after 197 timesteps\n",
            "[    0.  -300.  -100.  -200. -1000.     0.     0.     0.     0.     0.\n",
            "     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eWpCU8xH0ZKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ]
    },
    {
      "metadata": {
        "id": "7NxTojLi0N0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import json\n",
        "\n",
        "class Baseline():\n",
        "\n",
        "  def __init__(self, env, verbose=1):\n",
        "    self.env = env\n",
        "    self.verbose = verbose\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.map = self.env.map\n",
        "    self.rewards = self.env.customer_reward.copy()\n",
        "    \n",
        "  def as_string(self, state):\n",
        "    # reward/cost does not hurt, but is useless, path obsucres same state\n",
        "    new_state = {\n",
        "        'rewards': state['rewards'],\n",
        "        'position': state['position']\n",
        "    }\n",
        "    return json.dumps(new_state, sort_keys=True)\n",
        "  \n",
        "  def is_goal(self, state):\n",
        "    if state['position'] != 'S': return False\n",
        "    for reward in state['rewards'].values():\n",
        "      if reward != 0: return False\n",
        "    return True\n",
        "    \n",
        "\n",
        "  def expand(self, state):\n",
        "    states = []\n",
        "    for position, cost in self.map[state['position']]:\n",
        "      new_state = deepcopy(state)\n",
        "      new_state['position'] = position\n",
        "      new_state['rewards'][position] = 0\n",
        "      reward = state['rewards'][position]\n",
        "      new_state['reward'] += reward\n",
        "      new_state['cost'] += cost\n",
        "      new_state['path'].append(position)\n",
        "      states.append(new_state)\n",
        "    return states\n",
        "\n",
        "  def search(self, root, max_depth = 25):\n",
        "      closed = set()\n",
        "      open = [root]\n",
        "\n",
        "      while open:\n",
        "          state = open.pop(0)\n",
        "          if self.as_string(state) in closed: continue  \n",
        "\n",
        "          closed.add(self.as_string(state))\n",
        "\n",
        "          depth = len(state['path'])\n",
        "          if depth > max_depth:\n",
        "            if self.verbose > 0:\n",
        "              print(\"Visited:\", len(closed))\n",
        "              print(\"Reached max depth, without reaching goal\")\n",
        "            return None\n",
        "\n",
        "          if self.is_goal(state):\n",
        "            scaled_reward = (state['reward'] - state['cost']) / 6000\n",
        "            state['scaled_reward'] = scaled_reward\n",
        "            if self.verbose > 0:\n",
        "              print(\"Scaled reward:\", scaled_reward)            \n",
        "              print(\"Perfect path\", state['path'])\n",
        "            return state\n",
        "\n",
        "          expanded = self.expand(state)\n",
        "          open += expanded\n",
        "          # make this best first\n",
        "          open.sort(key=lambda state: state['cost'])\n",
        "        \n",
        "  def find_optimum(self):\n",
        "    initial_state = {\n",
        "        'rewards': self.rewards.copy(),\n",
        "        'position': 'S',\n",
        "        'reward': 0,\n",
        "        'cost': 0,\n",
        "        'path': ['S']\n",
        "    }\n",
        "    return self.search(initial_state)\n",
        "  \n",
        "  def benchmark(self, model, sample_runs=100):\n",
        "    self.verbose = 0\n",
        "    BeraterEnv.showStep = False\n",
        "    BeraterEnv.showDone = False\n",
        "\n",
        "    perfect_rewards = []\n",
        "    model_rewards = []\n",
        "    for run in range(sample_runs):\n",
        "      observation = self.env.reset()\n",
        "      self.reset()\n",
        "      \n",
        "      optimum_state = self.find_optimum()\n",
        "      perfect_rewards.append(optimum_state['scaled_reward'])\n",
        "      \n",
        "      state = np.zeros((1, 2*128))\n",
        "      dones = np.zeros((1))\n",
        "\n",
        "      for t in range(1000):\n",
        "        actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "        observation, reward, done, info = self.env.step(actions[0])\n",
        "        if done:\n",
        "          break\n",
        "      model_rewards.append(env.totalReward)\n",
        "    return perfect_rewards, model_rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4GlYjZ3xkQ38"
      },
      "cell_type": "markdown",
      "source": [
        "# Train model\n",
        "\n",
        "Estimation\n",
        "* total cost when travelling all paths (back and forth): 2500\n",
        "* all rewards: 6000\n",
        "* but: rewards are much more sparse while routes stay the same, maybe expect less\n",
        "* estimate: no illegal moves and between\n",
        "  * half the travel cost: (6000 - 1250) / 6000 = .79\n",
        "  * and full traval cost (6000 - 2500) / 6000 = 0.58\n",
        "* additionally: the agent only sees very little of the whole scenario\n",
        "  * changes with every episode\n",
        "  * was ok when network can learn fixed scenario\n"
      ]
    },
    {
      "metadata": {
        "id": "-rAaTCL0r-ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r logs\n",
        "!mkdir logs\n",
        "!mkdir logs/berater"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LArM6BsJgUvL",
        "colab_type": "code",
        "outputId": "a5280c13-2b58-4d96-99d4-a58c5aa7fba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GCufDIpnjNms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1: Extract MLP builder from openai sources"
      ]
    },
    {
      "metadata": {
        "id": "WMylk8s_amq1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# copied from https://github.com/openai/baselines/blob/master/baselines/a2c/utils.py\n",
        "\n",
        "def ortho_init(scale=1.0):\n",
        "    def _ortho_init(shape, dtype, partition_info=None):\n",
        "        #lasagne ortho init for tf\n",
        "        shape = tuple(shape)\n",
        "        if len(shape) == 2:\n",
        "            flat_shape = shape\n",
        "        elif len(shape) == 4: # assumes NHWC\n",
        "            flat_shape = (np.prod(shape[:-1]), shape[-1])\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
        "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
        "        q = u if u.shape == flat_shape else v # pick the one with the correct shape\n",
        "        q = q.reshape(shape)\n",
        "        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)\n",
        "    return _ortho_init      \n",
        "\n",
        "def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):\n",
        "    with tf.variable_scope(scope):\n",
        "        nin = x.get_shape()[1].value\n",
        "        w = tf.get_variable(\"w\", [nin, nh], initializer=ortho_init(init_scale))\n",
        "        b = tf.get_variable(\"b\", [nh], initializer=tf.constant_initializer(init_bias))\n",
        "        return tf.matmul(x, w)+b\n",
        "      \n",
        "\n",
        "# copied from https://github.com/openai/baselines/blob/master/baselines/common/models.py#L31\n",
        "def mlp(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n",
        "    \"\"\"\n",
        "    Stack of fully-connected layers to be used in a policy / q-function approximator\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "\n",
        "    num_layers: int                 number of fully-connected layers (default: 2)\n",
        "\n",
        "    num_hidden: int                 size of fully-connected layers (default: 64)\n",
        "\n",
        "    activation:                     activation function (default: tf.tanh)\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "\n",
        "    function that builds fully connected network with a given input tensor / placeholder\n",
        "    \"\"\"\n",
        "    def network_fn(X):\n",
        "#         print('network_fn called')\n",
        "#         Tensor(\"ppo2_model_4/Ob:0\", shape=(1, 19), dtype=float32)\n",
        "#         Tensor(\"ppo2_model_4/Ob_1:0\", shape=(512, 19), dtype=float32)\n",
        "#         print (X)\n",
        "        h = tf.layers.flatten(X)\n",
        "        for i in range(num_layers):\n",
        "            h = fc(h, 'mlp_fc{}'.format(i), nh=num_hidden, init_scale=np.sqrt(2))\n",
        "            if layer_norm:\n",
        "                h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n",
        "            h = activation(h)\n",
        "          \n",
        "#         Tensor(\"ppo2_model_4/pi/Tanh_2:0\", shape=(1, 500), dtype=float32)\n",
        "#         Tensor(\"ppo2_model_4/pi_2/Tanh_2:0\", shape=(512, 500), dtype=float32)\n",
        "#         print(h)\n",
        "        return h\n",
        "\n",
        "    return network_fn\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUvTLKKK8L8K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2: Replace exotic parts with standard tf layers\n",
        "* https://www.tensorflow.org/api_docs/python/tf/layers\n",
        "* https://www.tensorflow.org/api_docs/python/tf/layers/Dense"
      ]
    },
    {
      "metadata": {
        "id": "9X4G6Y4O8Khh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# first the dense layer\n",
        "def mlp_layers(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n",
        "    def network_fn(X):\n",
        "        h = tf.layers.flatten(X)\n",
        "        for i in range(num_layers):\n",
        "            h = tf.layers.dense(h, units=num_hidden, kernel_initializer=ortho_init(np.sqrt(2)))\n",
        "#             h = fc(h, 'mlp_fc{}'.format(i), nh=num_hidden, init_scale=np.sqrt(2))\n",
        "            if layer_norm:\n",
        "                h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n",
        "            h = activation(h)\n",
        "        return h\n",
        "\n",
        "    return network_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NIbexm3U_341",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# then the initializer (best practice glorot uniform works, but does not give reliable results, maybe use seed?)\n",
        "def mlp_layers(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n",
        "    def network_fn(X):\n",
        "        h = tf.layers.flatten(X)\n",
        "        for i in range(num_layers):\n",
        "#             h = tf.layers.dense(h, units=num_hidden, kernel_initializer=tf.initializers.glorot_uniform())\n",
        "            h = tf.layers.dense(h, units=num_hidden, kernel_initializer=tf.initializers.orthogonal(gain=np.sqrt(2)))\n",
        "            if layer_norm:\n",
        "                h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n",
        "            h = activation(h)\n",
        "        return h\n",
        "\n",
        "    return network_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fl91x05NHe1u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# standard batch normalization trains with none of the initializers, so we need to keep layer normalization\n",
        "def mlp_layers(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n",
        "    def network_fn(X):\n",
        "        h = tf.layers.flatten(X)\n",
        "        for i in range(num_layers):\n",
        "#             h = tf.layers.dense(h, units=num_hidden, kernel_initializer=tf.initializers.glorot_uniform(seed=42))\n",
        "            h = tf.layers.dense(h, units=num_hidden, kernel_initializer=tf.initializers.glorot_uniform())\n",
        "#             h = tf.layers.dense(h, units=num_hidden, kernel_initializer=tf.initializers.orthogonal(gain=np.sqrt(2)))\n",
        "            if layer_norm:\n",
        "#                 h = tf.layers.batch_normalization(h, center=True, scale=True)\n",
        "                h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n",
        "            h = activation(h)\n",
        "        return h\n",
        "\n",
        "    return network_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCyyWCZCjWbs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Option 2: Reimplement in Keras without any change\n",
        "* using Keras within TensorFlow model: https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html\n",
        "* https://stackoverflow.com/questions/46790506/calling-a-keras-model-on-a-tensorflow-tensor-but-keep-weights\n",
        "* https://stackoverflow.com/questions/38292760/tensorflow-introducing-both-l2-regularization-and-dropout-into-the-network-do\n",
        "* https://www.tensorflow.org/api_docs/python/tf/get_default_session\n",
        "* https://www.tensorflow.org/api_docs/python/tf/keras/backend/set_session"
      ]
    },
    {
      "metadata": {
        "id": "MNPMIIRYjVm5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras.initializers import Orthogonal\n",
        "\n",
        "def mlp_keras(num_layers=2, num_hidden=64, activation=tf.tanh, layer_norm=False):\n",
        "  def network_fn(X):\n",
        "#     print('network_fn called')\n",
        "#     Tensor(\"ppo2_model_3/Ob:0\", shape=(1, 19), dtype=float32)\n",
        "#     Tensor(\"ppo2_model_3/Ob_1:0\", shape=(512, 19), dtype=float32)\n",
        "#     print (X)\n",
        "\n",
        "#       should not matter at all\n",
        "    h = Flatten()(X)\n",
        "#     h = tf.layers.flatten(X)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "      # init taken from https://github.com/openai/baselines/blob/master/baselines/a2c/utils.py\n",
        "#       h = Dense(units=num_hidden, bias_initializer='zeros', kernel_initializer=Orthogonal(gain=np.sqrt(2)), name='mlp_fc{}'.format(i))(h)\n",
        "      h = Dense(units=num_hidden, bias_initializer='zeros', kernel_initializer=Orthogonal(gain=np.sqrt(2)), name='mlp_fc{}'.format(i))(h)\n",
        "      if layer_norm:\n",
        "        # we only have batch normalization in Keras\n",
        "#         https://arxiv.org/abs/1502.03167\n",
        "#         which is not quite what is described here\n",
        "        # https://arxiv.org/abs/1607.06450\n",
        "#         as we get horrible results with pure batch normalization, we use this one here\n",
        "#         https://www.tensorflow.org/api_docs/python/tf/contrib/layers/layer_norm\n",
        "        h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n",
        "#         h = BatchNormalization(center=True, scale=True)(h)\n",
        "      h = Activation(activation=activation)(h)\n",
        "    \n",
        "#     Tensor(\"ppo2_model_3/pi/activation_2/Tanh:0\", shape=(1, 500), dtype=float32)\n",
        "#     Tensor(\"ppo2_model_3/pi_2/activation_5/Tanh:0\", shape=(512, 500), dtype=float32)\n",
        "#     print(h)\n",
        "    return h\n",
        "\n",
        "  return network_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NzbylmYAkQ3-",
        "outputId": "3720ddac-b4af-4967-ce51-9272524dae48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2897
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# https://github.com/openai/baselines/blob/master/baselines/deepq/experiments/train_pong.py\n",
        "# log_dir = logger.get_dir()\n",
        "log_dir = '/content/logs/berater/'\n",
        "\n",
        "import gym\n",
        "from baselines import bench\n",
        "from baselines import logger\n",
        "\n",
        "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
        "from baselines.common.vec_env.vec_monitor import VecMonitor\n",
        "from baselines.ppo2 import ppo2\n",
        "\n",
        "BeraterEnv.showStep = False\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "env = BeraterEnv()\n",
        "\n",
        "wrapped_env = DummyVecEnv([lambda: BeraterEnv()])\n",
        "monitored_env = VecMonitor(wrapped_env, log_dir)\n",
        "\n",
        "# https://github.com/openai/baselines/blob/master/baselines/ppo2/ppo2.py\n",
        "# https://github.com/openai/baselines/blob/master/baselines/common/models.py#L30\n",
        "# https://arxiv.org/abs/1607.06450 for layer_norm\n",
        "\n",
        "# lr linear from lr=1e-2 to lr=1e-4 (default lr=3e-4)\n",
        "def lr_range(frac):\n",
        "  # we get the remaining updates between 1 and 0\n",
        "  start_lr = 1e-2\n",
        "  end_lr = 1e-4\n",
        "  diff_lr = start_lr - end_lr\n",
        "  lr = end_lr + diff_lr * frac\n",
        "  return lr\n",
        "  \n",
        "# network = mlp(num_hidden=500, num_layers=3, layer_norm=True)\n",
        "network = mlp_layers(num_hidden=500, num_layers=3, layer_norm=True)\n",
        "  \n",
        "model = ppo2.learn(\n",
        "    env=monitored_env,\n",
        "    network=network,\n",
        "    lr=lr_range,\n",
        "    gamma=1.0,\n",
        "    ent_coef=0.05,\n",
        "    total_timesteps=500000)\n",
        "\n",
        "# model = ppo2.learn(\n",
        "#     env=monitored_env,\n",
        "#     network='mlp',\n",
        "#     num_hidden=500,\n",
        "#     num_layers=3,\n",
        "#     layer_norm=True,\n",
        "#     lr=lr_range,\n",
        "#     gamma=1.0,\n",
        "#     ent_coef=0.05,\n",
        "#     total_timesteps=500000)\n",
        "\n",
        "\n",
        "# model.save('berater-ppo-v11.pkl')\n",
        "monitored_env.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logging to /tmp/openai-2019-01-24-09-39-58-169577\n",
            "-----------------------------------\n",
            "| approxkl           | 17.994278  |\n",
            "| clipfrac           | 0.8354492  |\n",
            "| eplenmean          | 139        |\n",
            "| eprewmean          | -8.29487   |\n",
            "| explained_variance | -0.386     |\n",
            "| fps                | 430        |\n",
            "| nupdates           | 1          |\n",
            "| policy_entropy     | 0.6977349  |\n",
            "| policy_loss        | 0.21713948 |\n",
            "| serial_timesteps   | 2048       |\n",
            "| time_elapsed       | 4.76       |\n",
            "| total_timesteps    | 2048       |\n",
            "| value_loss         | 4.3987803  |\n",
            "-----------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 8.8567836e-13 |\n",
            "| clipfrac           | 0.0           |\n",
            "| eplenmean          | 148           |\n",
            "| eprewmean          | -9.013093     |\n",
            "| explained_variance | -0.0116       |\n",
            "| fps                | 456           |\n",
            "| nupdates           | 10            |\n",
            "| policy_entropy     | 7.215558e-05  |\n",
            "| policy_loss        | 1.1869997e-07 |\n",
            "| serial_timesteps   | 20480         |\n",
            "| time_elapsed       | 45.2          |\n",
            "| total_timesteps    | 20480         |\n",
            "| value_loss         | 0.033989336   |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| approxkl           | 0.43005157  |\n",
            "| clipfrac           | 0.5806885   |\n",
            "| eplenmean          | 148         |\n",
            "| eprewmean          | -9.013093   |\n",
            "| explained_variance | -0.000793   |\n",
            "| fps                | 458         |\n",
            "| nupdates           | 20          |\n",
            "| policy_entropy     | 0.43354544  |\n",
            "| policy_loss        | 0.072340146 |\n",
            "| serial_timesteps   | 40960       |\n",
            "| time_elapsed       | 89.9        |\n",
            "| total_timesteps    | 40960       |\n",
            "| value_loss         | 3.4269454   |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| approxkl           | 0.046057116  |\n",
            "| clipfrac           | 0.026855469  |\n",
            "| eplenmean          | 148          |\n",
            "| eprewmean          | -9.013093    |\n",
            "| explained_variance | -8.89e-05    |\n",
            "| fps                | 457          |\n",
            "| nupdates           | 30           |\n",
            "| policy_entropy     | 0.057752084  |\n",
            "| policy_loss        | -0.007410792 |\n",
            "| serial_timesteps   | 61440        |\n",
            "| time_elapsed       | 135          |\n",
            "| total_timesteps    | 61440        |\n",
            "| value_loss         | 0.39356554   |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| approxkl           | 6.201327e-11  |\n",
            "| clipfrac           | 0.0           |\n",
            "| eplenmean          | 148           |\n",
            "| eprewmean          | -9.013093     |\n",
            "| explained_variance | -8.11e-06     |\n",
            "| fps                | 454           |\n",
            "| nupdates           | 40            |\n",
            "| policy_entropy     | 0.00036115825 |\n",
            "| policy_loss        | 8.070492e-08  |\n",
            "| serial_timesteps   | 81920         |\n",
            "| time_elapsed       | 180           |\n",
            "| total_timesteps    | 81920         |\n",
            "| value_loss         | 0.3274109     |\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bfae2ecd9e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n# https://github.com/openai/baselines/blob/master/baselines/deepq/experiments/train_pong.py\\n# log_dir = logger.get_dir()\\nlog_dir = '/content/logs/berater/'\\n\\nimport gym\\nfrom baselines import bench\\nfrom baselines import logger\\n\\nfrom baselines.common.vec_env.dummy_vec_env import DummyVecEnv\\nfrom baselines.common.vec_env.vec_monitor import VecMonitor\\nfrom baselines.ppo2 import ppo2\\n\\nBeraterEnv.showStep = False\\nBeraterEnv.showDone = False\\n\\nenv = BeraterEnv()\\n\\nwrapped_env = DummyVecEnv([lambda: BeraterEnv()])\\nmonitored_env = VecMonitor(wrapped_env, log_dir)\\n\\n# https://github.com/openai/baselines/blob/master/baselines/ppo2/ppo2.py\\n# https://github.com/openai/baselines/blob/master/baselines/common/models.py#L30\\n# https://arxiv.org/abs/1607.06450 for layer_norm\\n\\n# lr linear from lr=1e-2 to lr=1e-4 (default lr=3e-4)\\ndef lr_range(frac):\\n  # we get the remaining updates between 1 and 0\\n  start_lr = 1e-2\\n  end_lr = 1e-4\\n  diff_lr = start_lr - end_lr\\n  lr = end_lr + diff_lr * frac\\n  return lr\\n  \\n# network = mlp(num_hidden=500, num_layers=3, layer_norm=True)\\nnetwork = mlp_layers(num_hidden=500, num_layers=3, layer_norm=True)\\n  \\nmodel = ppo2.learn(\\n    env=monitored_env,\\n    network=network,\\n    lr=lr_range,\\n    gamma=1.0,\\n    ent_coef=0.05,\\n    total_timesteps=500000)\\n\\n#...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(network, env, total_timesteps, eval_env, seed, nsteps, ent_coef, lr, vf_coef, max_grad_norm, gamma, lam, log_interval, nminibatches, noptepochs, cliprange, save_interval, load_path, model_fn, **network_kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mmbinds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                     \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmbinds\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     \u001b[0mmblossvals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrnow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcliprangenow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# recurrent version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mnenvs\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnminibatches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/baselines/ppo2/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, cliprange, obs, returns, masks, actions, values, neglogpacs, states)\u001b[0m\n\u001b[1;32m    152\u001b[0m         return self.sess.run(\n\u001b[1;32m    153\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtd_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         )[:-1]\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "0cfzto7W8Mpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing Results\n",
        "\n",
        "https://github.com/openai/baselines/blob/master/docs/viz/viz.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "yBzvtyVcvhkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !ls -l $log_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZWB88EVsRei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from baselines.common import plot_util as pu\n",
        "results = pu.load_results(log_dir)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "r = results[0]\n",
        "plt.ylim(0, .75)\n",
        "# plt.plot(np.cumsum(r.monitor.l), r.monitor.r)\n",
        "plt.plot(np.cumsum(r.monitor.l), pu.smooth(r.monitor.r, radius=100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TtBh4c6-kQ4K"
      },
      "cell_type": "markdown",
      "source": [
        "# Enjoy model"
      ]
    },
    {
      "metadata": {
        "id": "H_QTckfBra7l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "observation = env.reset()\n",
        "env.render()\n",
        "baseline = Baseline(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ucP0gNhhkQ4O",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state = np.zeros((1, 2*128))\n",
        "dones = np.zeros((1))\n",
        "\n",
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "for t in range(1000):\n",
        "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "    observation, reward, done, info = env.step(actions[0])\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, reward={}\".format(t+1, env.totalReward))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3z35_dMMt6SW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%time baseline.find_optimum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K36GXkzyRGOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "KMb58O_q067F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "baseline = Baseline(env)\n",
        "optimum_score, model_score = baseline.benchmark(model, sample_runs=100)\n",
        "optimum_score, model_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dr9ylHgnRIcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.array(optimum_score).mean(), np.array(optimum_score).std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOSOoO29Rwgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.array(model_score).mean(), np.array(model_score).std()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ls8IKVV1R5SE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}