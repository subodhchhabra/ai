{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "berater-v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater-v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v4\n",
        "\n",
        "## Changes from v3\n",
        "1. removed confusing log statements in environment\n",
        "1. switched to policy gradient\n",
        "\n",
        "## Next Steps\n",
        "1. Make monitor files work and plot performance\n",
        "1. choose costs of traversal randomly with each episode\n",
        "  * aim: agent will (hopefully) be able to work with any costs\n",
        "1. train a different graph with each episode \n",
        "  * aim: agent can work on any graph"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Installation (required for colab)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/baselines >/dev/null\n",
        "!pip install gym >/dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7Y3XKzw1kQ3I",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cnt=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n",
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 3 discrete deterministic actions:\n",
        "    - 0: First Direction\n",
        "    - 1: Second Direction\n",
        "    - 2: Third Direction / Go home\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    num_envs = 1\n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    showRender = False\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "        self.map = {\n",
        "            'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "            'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "            'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "            'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "        }\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=numpy.array([0,-1000,-1000,-1000,-1000,-1000,-1000]),\n",
        "                                             high=numpy.array([3,1000,1000,1000,1000,1000,1000]),\n",
        "                                             dtype=numpy.float32)\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, actionArg):\n",
        "        paths = self.map[self.state]\n",
        "        action = actionArg\n",
        "        destination, cost = paths[action]\n",
        "        lastState = self.state\n",
        "        lastObState = state_name_to_int(lastState)\n",
        "        customerReward = self.customer_reward[destination]\n",
        "\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        self.state = destination\n",
        "        reward = (-cost + self.customer_reward[destination]) / self.optimum\n",
        "        self.customer_visited(destination)\n",
        "        done = destination == 'S' and self.all_customers_visited()\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        \n",
        "        \n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   #lastState + ':' + str(lastObState) + ' --' + str(action) + '-> ' + self.state + ':' + str(stateAsInt) +\n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = numpy.array([ position, \n",
        "                               self.getEdgeObservation('S','A'),\n",
        "                               self.getEdgeObservation('S','B'),\n",
        "                               self.getEdgeObservation('S','C'),\n",
        "                               self.getEdgeObservation('A','B'),\n",
        "                               self.getEdgeObservation('A','C'),\n",
        "                               self.getEdgeObservation('B','C'),\n",
        "                              ],\n",
        "                             dtype=numpy.float32)\n",
        "        return result\n",
        "\n",
        "    def getEdgeObservation(self, source, target):\n",
        "        reward = self.customer_reward[target] \n",
        "        cost = self.getCost(source,target)\n",
        "        result = reward - cost\n",
        "\n",
        "        return result\n",
        "\n",
        "    def getCost(self, source, target):\n",
        "        paths = self.map[source]\n",
        "        targetIndex=state_name_to_int(target)\n",
        "        for destination, cost in paths:\n",
        "            if destination == target:\n",
        "                result = cost\n",
        "                break\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "        reward_per_customer = 1000\n",
        "        self.customer_reward = {\n",
        "            'S': 0,\n",
        "            'A': reward_per_customer,\n",
        "            'B': reward_per_customer,\n",
        "            'C': reward_per_customer,\n",
        "        }\n",
        "\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        if BeraterEnv.showRender:\n",
        "            print( (\"steps=%4.0f  \" % self.stepCount) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + ' done=' + str(self.isDone))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kWF5vSsakQ3b"
      },
      "cell_type": "markdown",
      "source": [
        "# Register Einvornment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SU1NxPMokQ3e",
        "outputId": "5682d178-968c-4edc-90c9-250d46057a30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from gym.envs.registration import register\n",
        "\n",
        "cnt += 1\n",
        "id = \"Berater-v{}\".format(cnt)\n",
        "register(\n",
        "    id=id,\n",
        "    entry_point=BeraterEnv\n",
        ")   \n",
        "\n",
        "print(id)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater-v1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Usj9iWTskQ3t"
      },
      "cell_type": "markdown",
      "source": [
        "# Try out Environment"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oTtUfeONkQ3w",
        "outputId": "16fa5a2c-7ced-4524-ce31-ee67453f0127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = True\n",
        "\n",
        "env = gym.make(id)\n",
        "observation = env.reset()\n",
        "print(env)\n",
        "\n",
        "for t in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        env.render()\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BeraterEnv<Berater-v1>>\n",
            "Episode:    0   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    2  A --1-> C R= 0.20 totalR= 0.50 cost= 400 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    3  C --0-> A R=-0.13 totalR= 0.37 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    4  A --1-> C R=-0.13 totalR= 0.23 cost= 400 customerR=   0 optimum=3000\n",
            "Episode:    0   Step:    5  C --1-> B R= 0.25 totalR= 0.48 cost= 250 customerR=1000 optimum=3000\n",
            "Episode:    0   Step:    6  B --2-> S R=-0.13 totalR= 0.35 cost= 400 customerR=   0 optimum=3000\n",
            "Done: episodes=     1  avgSteps=  6.00  avgTotalReward= 0.35\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4GlYjZ3xkQ38"
      },
      "cell_type": "markdown",
      "source": [
        "# Train model\n",
        "\n",
        "* 0.73 would be perfect total reward"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NzbylmYAkQ3-",
        "outputId": "9b913ab0-0bb9-486e-87e7-085f85d29da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from baselines import deepq\n",
        "\n",
        "BeraterEnv.showStep = False\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "env = gym.make(id)\n",
        "\n",
        "# https://en.wikipedia.org/wiki/Q-learning#Influence_of_variables\n",
        "%time model = deepq.learn(\\\n",
        "        env,\\\n",
        "        seed=42,\\\n",
        "        network='mlp',\\\n",
        "        lr=1e-3,\\\n",
        "        total_timesteps=30000,\\\n",
        "        buffer_size=50000,\\\n",
        "        exploration_fraction=0.5,\\\n",
        "        exploration_final_eps=0.02,\\\n",
        "        print_freq=1000)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logging to /tmp/openai-2018-12-28-14-35-53-094162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------------------------\n",
            "| % time spent exploring  | 53       |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 0.5      |\n",
            "| steps                   | 7071     |\n",
            "--------------------------------------\n",
            "Saving model due to mean reward increase: None -> 0.6\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 21       |\n",
            "| episodes                | 2000     |\n",
            "| mean 100 episode reward | 0.6      |\n",
            "| steps                   | 12059    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 3000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 16237    |\n",
            "--------------------------------------\n",
            "Saving model due to mean reward increase: 0.6 -> 0.7\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 4000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 20288    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 5000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 24341    |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 6000     |\n",
            "| mean 100 episode reward | 0.7      |\n",
            "| steps                   | 28388    |\n",
            "--------------------------------------\n",
            "Restored model with mean reward: 0.7\n",
            "CPU times: user 6min 6s, sys: 51.2 s, total: 6min 57s\n",
            "Wall time: 4min 49s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TtBh4c6-kQ4K"
      },
      "cell_type": "markdown",
      "source": [
        "# Enjoy model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ucP0gNhhkQ4O",
        "outputId": "3fd2e781-a980-4488-9a43-cb2089c1880b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "observation = env.reset()\n",
        "state = np.zeros((1, 2*128))\n",
        "dones = np.zeros((1))\n",
        "\n",
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "for t in range(1000):\n",
        "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "    observation, reward, done, info = env.step(actions[0])\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 6401   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
            "Episode: 6401   Step:    2  A --0-> B R= 0.25 totalR= 0.55 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 6401   Step:    3  B --1-> C R= 0.25 totalR= 0.80 cost= 250 customerR=1000 optimum=3000\n",
            "Episode: 6401   Step:    4  C --2-> S R=-0.07 totalR= 0.73 cost= 200 customerR=   0 optimum=3000\n",
            "Episode finished after 4 timesteps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5fY1da_0l15E",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}