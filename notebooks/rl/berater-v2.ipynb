{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DJCordhose/ai/blob/master/notebooks/rl/berater_chh_181104.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eU7ylMh1kQ2y"
   },
   "source": [
    "# Berater Environment v2\n",
    "\n",
    "## Changes from v1\n",
    "1. change of observation space \n",
    "  * used to be just on discrete value: position on graph: ```spaces.Discrete(1)```\n",
    "  * give agent complete field including costs to enable learning: ```spaces.Box```\n",
    "\n",
    "## Next Steps\n",
    "1. choose costs of traversal randomly with each episode\n",
    "  * aim: agent will (hopefully) be able to work with any costs\n",
    "1. train a different graph with each episode \n",
    "  * aim: agent can work on any graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zpzHtN3-kQ26"
   },
   "source": [
    "## Installation (required for colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0E567zPTkQ28"
   },
   "outputs": [],
   "source": [
    "# !pip install -e git+https://github.com/openai/baselines#egg=berater"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2r0ZJnWJkfxL"
   },
   "source": [
    "### important for colab: comment line above and restart runtime after installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Y3XKzw1kQ3I"
   },
   "outputs": [],
   "source": [
    "cnt=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-S4sZG5ZkQ3T"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import gym\n",
    "from gym.utils import seeding\n",
    "from gym import spaces\n",
    "\n",
    "def state_name_to_int(state):\n",
    "    state_name_map = {\n",
    "        'S': 0,\n",
    "        'A': 1,\n",
    "        'B': 2,\n",
    "        'C': 3,\n",
    "    }\n",
    "    return state_name_map[state]\n",
    "\n",
    "def int_to_state_name(state_as_int):\n",
    "    state_map = {\n",
    "        0: 'S',\n",
    "        1: 'A',\n",
    "        2: 'B',\n",
    "        3: 'C'\n",
    "    }\n",
    "    return state_map[state_as_int]\n",
    "    \n",
    "class BeraterEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The Berater Problem\n",
    "\n",
    "    Actions: \n",
    "    There are 3 discrete deterministic actions:\n",
    "    - 0: First Direction\n",
    "    - 1: Second Direction\n",
    "    - 2: Third Direction / Go home\n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['ansi']}\n",
    "    \n",
    "    num_envs = 1\n",
    "    showStep = False\n",
    "    showDone = True\n",
    "    showRender = False\n",
    "    envEpisodeModulo = 100\n",
    "\n",
    "    def __init__(self):\n",
    "        self.map = {\n",
    "            'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
    "            'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
    "            'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
    "            'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
    "        }\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=numpy.array([0,-1000,-1000,-1000,-1000,-1000,-1000]),\n",
    "                                             high=numpy.array([3,1000,1000,1000,1000,1000,1000]),\n",
    "                                             dtype=numpy.float32)\n",
    "\n",
    "\n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "\n",
    "        self.envReward = 0\n",
    "        self.envEpisodeCount = 0\n",
    "        self.envStepCount = 0\n",
    "\n",
    "        self.reset()\n",
    "        self.optimum = self.calculate_customers_reward()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, actionArg):\n",
    "        paths = self.map[self.state]\n",
    "        action = actionArg\n",
    "        destination, cost = paths[action]\n",
    "        lastState = self.state\n",
    "        lastObState = state_name_to_int(lastState)\n",
    "        customerReward = self.customer_reward[destination]\n",
    "\n",
    "        info = {\"from\": self.state, \"to\": destination}\n",
    "\n",
    "        self.state = destination\n",
    "        reward = (-cost + self.customer_reward[destination]) / self.optimum\n",
    "        self.customer_visited(destination)\n",
    "        done = destination == 'S' and self.all_customers_visited()\n",
    "\n",
    "        stateAsInt = state_name_to_int(self.state)\n",
    "        self.totalReward += reward\n",
    "        self.stepCount += 1\n",
    "        self.envReward += reward\n",
    "        self.envStepCount += 1\n",
    "\n",
    "        if self.showStep:\n",
    "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
    "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
    "                   #lastState + ':' + str(lastObState) + ' --' + str(action) + '-> ' + self.state + ':' + str(stateAsInt) +\n",
    "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
    "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
    "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
    "                   )\n",
    "\n",
    "        if done and not self.isDone:\n",
    "            self.envEpisodeCount += 1\n",
    "            if BeraterEnv.showDone or (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
    "                episodes = BeraterEnv.envEpisodeModulo\n",
    "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
    "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
    "                print( \"Done: \" + \n",
    "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
    "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
    "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
    "                        )\n",
    "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
    "                    self.envReward = 0\n",
    "                    self.envStepCount = 0\n",
    "\n",
    "        self.isDone = done\n",
    "        observation = self.getObservation(stateAsInt)\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def getObservation(self, position):\n",
    "        result = numpy.array([ position, \n",
    "                               self.getEdgeObservation('S','A'),\n",
    "                               self.getEdgeObservation('S','B'),\n",
    "                               self.getEdgeObservation('S','C'),\n",
    "                               self.getEdgeObservation('A','B'),\n",
    "                               self.getEdgeObservation('A','C'),\n",
    "                               self.getEdgeObservation('B','C'),\n",
    "                              ],\n",
    "                             dtype=numpy.float32)\n",
    "        return result\n",
    "\n",
    "    def getEdgeObservation(self, source, target):\n",
    "        reward = self.customer_reward[target] \n",
    "        cost = self.getCost(source,target)\n",
    "        result = reward - cost\n",
    "\n",
    "        return result\n",
    "\n",
    "    def getCost(self, source, target):\n",
    "        paths = self.map[source]\n",
    "        targetIndex=state_name_to_int(target)\n",
    "        for destination, cost in paths:\n",
    "            if destination == target:\n",
    "                result = cost\n",
    "                break\n",
    "\n",
    "        return result\n",
    "\n",
    "    def customer_visited(self, customer):\n",
    "        self.customer_reward[customer] = 0\n",
    "\n",
    "    def all_customers_visited(self):\n",
    "        return self.calculate_customers_reward() == 0\n",
    "\n",
    "    def calculate_customers_reward(self):\n",
    "        sum = 0\n",
    "        for value in self.customer_reward.values():\n",
    "            sum += value\n",
    "        return sum\n",
    "\n",
    "    def reset(self):\n",
    "        # print(\"Reset\")\n",
    "        \n",
    "        self.totalReward = 0\n",
    "        self.stepCount = 0\n",
    "        self.isDone = False\n",
    "        reward_per_customer = 1000\n",
    "        self.customer_reward = {\n",
    "            'S': 0,\n",
    "            'A': reward_per_customer,\n",
    "            'B': reward_per_customer,\n",
    "            'C': reward_per_customer,\n",
    "        }\n",
    "\n",
    "        self.state = 'S'\n",
    "        return state_name_to_int(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if BeraterEnv.showRender:\n",
    "            print( (\"steps=%4.0f  \" % self.stepCount) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + ' done=' + str(self.isDone))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWF5vSsakQ3b"
   },
   "source": [
    "# Register Einvornment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SU1NxPMokQ3e",
    "outputId": "b9d3873a-a6d5-4207-8282-91a2d6d2a6a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berater-v1\n"
     ]
    }
   ],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "cnt += 1\n",
    "id = \"Berater-v{}\".format(cnt)\n",
    "register(\n",
    "    id=id,\n",
    "    entry_point=BeraterEnv\n",
    ")   \n",
    "\n",
    "print(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Usj9iWTskQ3t"
   },
   "source": [
    "# Try out Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "oTtUfeONkQ3w",
    "outputId": "3e450e90-b97c-479e-9d37-9e900b37988e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BeraterEnv<Berater-v1>>\n",
      "Episode:    0   Step:    1  S --0-> A R= 0.30 totalR= 0.30 cost= 100 customerR=1000 optimum=3000\n",
      "Episode:    0   Step:    2  A --1-> C R= 0.20 totalR= 0.50 cost= 400 customerR=1000 optimum=3000\n",
      "Episode:    0   Step:    3  C --0-> A R=-0.13 totalR= 0.37 cost= 400 customerR=   0 optimum=3000\n",
      "Episode:    0   Step:    4  A --1-> C R=-0.13 totalR= 0.23 cost= 400 customerR=   0 optimum=3000\n",
      "Episode:    0   Step:    5  C --1-> B R= 0.25 totalR= 0.48 cost= 250 customerR=1000 optimum=3000\n",
      "Episode:    0   Step:    6  B --2-> S R=-0.13 totalR= 0.35 cost= 400 customerR=   0 optimum=3000\n",
      "Done: episodes=     1  avgSteps=  6.00  avgTotalReward= 0.35\n"
     ]
    }
   ],
   "source": [
    "BeraterEnv.showStep = True\n",
    "BeraterEnv.showDone = True\n",
    "\n",
    "env = gym.make(id)\n",
    "observation = env.reset()\n",
    "print(env)\n",
    "\n",
    "for t in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.render()\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4GlYjZ3xkQ38"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2958
    },
    "colab_type": "code",
    "id": "NzbylmYAkQ3-",
    "outputId": "4ce797b9-ca76-4652-e5ab-225ef52a08b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2018-11-26-10-33-07-701445\n",
      "Done: episodes=   100  avgSteps=  8.18  avgTotalReward= 0.29\n",
      "Done: episodes=   200  avgSteps=  8.66  avgTotalReward= 0.22\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0018272153 |\n",
      "| clipfrac           | 0.020507812  |\n",
      "| eplenmean          | nan          |\n",
      "| eprewmean          | nan          |\n",
      "| explained_variance | -0.4         |\n",
      "| fps                | 430          |\n",
      "| nupdates           | 1            |\n",
      "| policy_entropy     | 1.0966616    |\n",
      "| policy_loss        | -0.014113387 |\n",
      "| serial_timesteps   | 2048         |\n",
      "| time_elapsed       | 4.75         |\n",
      "| total_timesteps    | 2048         |\n",
      "| value_loss         | 0.054653596  |\n",
      "-------------------------------------\n",
      "Done: episodes=   300  avgSteps=  8.43  avgTotalReward= 0.25\n",
      "Done: episodes=   400  avgSteps=  7.89  avgTotalReward= 0.29\n",
      "Done: episodes=   500  avgSteps=  7.78  avgTotalReward= 0.31\n",
      "Done: episodes=   600  avgSteps=  6.82  avgTotalReward= 0.39\n",
      "Done: episodes=   700  avgSteps=  6.90  avgTotalReward= 0.40\n",
      "Done: episodes=   800  avgSteps=  7.13  avgTotalReward= 0.38\n",
      "Done: episodes=   900  avgSteps=  6.00  avgTotalReward= 0.46\n",
      "Done: episodes=  1000  avgSteps=  6.49  avgTotalReward= 0.43\n",
      "Done: episodes=  1100  avgSteps=  6.29  avgTotalReward= 0.44\n",
      "Done: episodes=  1200  avgSteps=  6.24  avgTotalReward= 0.45\n",
      "Done: episodes=  1300  avgSteps=  6.23  avgTotalReward= 0.47\n",
      "Done: episodes=  1400  avgSteps=  5.91  avgTotalReward= 0.48\n",
      "Done: episodes=  1500  avgSteps=  6.09  avgTotalReward= 0.45\n",
      "Done: episodes=  1600  avgSteps=  5.92  avgTotalReward= 0.47\n",
      "Done: episodes=  1700  avgSteps=  5.95  avgTotalReward= 0.46\n",
      "Done: episodes=  1800  avgSteps=  6.05  avgTotalReward= 0.46\n",
      "Done: episodes=  1900  avgSteps=  5.20  avgTotalReward= 0.54\n",
      "Done: episodes=  2000  avgSteps=  5.39  avgTotalReward= 0.52\n",
      "Done: episodes=  2100  avgSteps=  5.23  avgTotalReward= 0.53\n",
      "Done: episodes=  2200  avgSteps=  5.42  avgTotalReward= 0.52\n",
      "Done: episodes=  2300  avgSteps=  5.07  avgTotalReward= 0.54\n",
      "Done: episodes=  2400  avgSteps=  5.18  avgTotalReward= 0.54\n",
      "Done: episodes=  2500  avgSteps=  5.38  avgTotalReward= 0.52\n",
      "Done: episodes=  2600  avgSteps=  5.30  avgTotalReward= 0.53\n",
      "Done: episodes=  2700  avgSteps=  5.01  avgTotalReward= 0.55\n",
      "Done: episodes=  2800  avgSteps=  4.91  avgTotalReward= 0.57\n",
      "Done: episodes=  2900  avgSteps=  5.07  avgTotalReward= 0.54\n",
      "Done: episodes=  3000  avgSteps=  4.96  avgTotalReward= 0.55\n",
      "Done: episodes=  3100  avgSteps=  4.87  avgTotalReward= 0.57\n",
      "Done: episodes=  3200  avgSteps=  5.05  avgTotalReward= 0.53\n",
      "Done: episodes=  3300  avgSteps=  4.93  avgTotalReward= 0.55\n",
      "Done: episodes=  3400  avgSteps=  4.84  avgTotalReward= 0.58\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0015142018 |\n",
      "| clipfrac           | 0.013671875  |\n",
      "| eplenmean          | nan          |\n",
      "| eprewmean          | nan          |\n",
      "| explained_variance | 0.755        |\n",
      "| fps                | 504          |\n",
      "| nupdates           | 10           |\n",
      "| policy_entropy     | 0.7535325    |\n",
      "| policy_loss        | -0.014194089 |\n",
      "| serial_timesteps   | 20480        |\n",
      "| time_elapsed       | 40.7         |\n",
      "| total_timesteps    | 20480        |\n",
      "| value_loss         | 0.00904465   |\n",
      "-------------------------------------\n",
      "Done: episodes=  3500  avgSteps=  4.80  avgTotalReward= 0.59\n",
      "Done: episodes=  3600  avgSteps=  4.75  avgTotalReward= 0.59\n",
      "Done: episodes=  3700  avgSteps=  4.60  avgTotalReward= 0.59\n",
      "Done: episodes=  3800  avgSteps=  4.63  avgTotalReward= 0.57\n",
      "Done: episodes=  3900  avgSteps=  4.67  avgTotalReward= 0.59\n",
      "Done: episodes=  4000  avgSteps=  4.60  avgTotalReward= 0.61\n",
      "Done: episodes=  4100  avgSteps=  4.56  avgTotalReward= 0.60\n",
      "Done: episodes=  4200  avgSteps=  4.70  avgTotalReward= 0.59\n",
      "Done: episodes=  4300  avgSteps=  4.40  avgTotalReward= 0.63\n",
      "Done: episodes=  4400  avgSteps=  4.43  avgTotalReward= 0.64\n",
      "Done: episodes=  4500  avgSteps=  4.40  avgTotalReward= 0.62\n",
      "Done: episodes=  4600  avgSteps=  4.49  avgTotalReward= 0.61\n",
      "Done: episodes=  4700  avgSteps=  4.37  avgTotalReward= 0.65\n",
      "Done: episodes=  4800  avgSteps=  4.40  avgTotalReward= 0.63\n",
      "Done: episodes=  4900  avgSteps=  4.32  avgTotalReward= 0.64\n",
      "Done: episodes=  5000  avgSteps=  4.48  avgTotalReward= 0.63\n",
      "Done: episodes=  5100  avgSteps=  4.42  avgTotalReward= 0.63\n",
      "Done: episodes=  5200  avgSteps=  4.32  avgTotalReward= 0.63\n",
      "Done: episodes=  5300  avgSteps=  4.32  avgTotalReward= 0.64\n",
      "Done: episodes=  5400  avgSteps=  4.35  avgTotalReward= 0.65\n",
      "Done: episodes=  5500  avgSteps=  4.23  avgTotalReward= 0.66\n",
      "Done: episodes=  5600  avgSteps=  4.26  avgTotalReward= 0.64\n",
      "Done: episodes=  5700  avgSteps=  4.32  avgTotalReward= 0.63\n",
      "Done: episodes=  5800  avgSteps=  4.22  avgTotalReward= 0.66\n",
      "Done: episodes=  5900  avgSteps=  4.16  avgTotalReward= 0.67\n",
      "Done: episodes=  6000  avgSteps=  4.13  avgTotalReward= 0.68\n",
      "Done: episodes=  6100  avgSteps=  4.26  avgTotalReward= 0.66\n",
      "Done: episodes=  6200  avgSteps=  4.24  avgTotalReward= 0.66\n",
      "Done: episodes=  6300  avgSteps=  4.14  avgTotalReward= 0.68\n",
      "Done: episodes=  6400  avgSteps=  4.17  avgTotalReward= 0.68\n",
      "Done: episodes=  6500  avgSteps=  4.25  avgTotalReward= 0.67\n",
      "Done: episodes=  6600  avgSteps=  4.10  avgTotalReward= 0.68\n",
      "Done: episodes=  6700  avgSteps=  4.15  avgTotalReward= 0.68\n",
      "Done: episodes=  6800  avgSteps=  4.17  avgTotalReward= 0.69\n",
      "Done: episodes=  6900  avgSteps=  4.12  avgTotalReward= 0.69\n",
      "Done: episodes=  7000  avgSteps=  4.17  avgTotalReward= 0.68\n",
      "Done: episodes=  7100  avgSteps=  4.18  avgTotalReward= 0.69\n",
      "Done: episodes=  7200  avgSteps=  4.16  avgTotalReward= 0.69\n",
      "Done: episodes=  7300  avgSteps=  4.11  avgTotalReward= 0.70\n",
      "Done: episodes=  7400  avgSteps=  4.18  avgTotalReward= 0.69\n",
      "Done: episodes=  7500  avgSteps=  4.13  avgTotalReward= 0.69\n",
      "Done: episodes=  7600  avgSteps=  4.16  avgTotalReward= 0.68\n",
      "Done: episodes=  7700  avgSteps=  4.10  avgTotalReward= 0.70\n",
      "Done: episodes=  7800  avgSteps=  4.08  avgTotalReward= 0.71\n",
      "Done: episodes=  7900  avgSteps=  4.10  avgTotalReward= 0.70\n",
      "Done: episodes=  8000  avgSteps=  4.07  avgTotalReward= 0.70\n",
      "Done: episodes=  8100  avgSteps=  4.07  avgTotalReward= 0.71\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0010771031 |\n",
      "| clipfrac           | 0.013793945  |\n",
      "| eplenmean          | nan          |\n",
      "| eprewmean          | nan          |\n",
      "| explained_variance | 0.975        |\n",
      "| fps                | 511          |\n",
      "| nupdates           | 20           |\n",
      "| policy_entropy     | 0.34031704   |\n",
      "| policy_loss        | -0.017940775 |\n",
      "| serial_timesteps   | 40960        |\n",
      "| time_elapsed       | 80.6         |\n",
      "| total_timesteps    | 40960        |\n",
      "| value_loss         | 0.0010044485 |\n",
      "-------------------------------------\n",
      "Done: episodes=  8200  avgSteps=  4.07  avgTotalReward= 0.71\n",
      "Done: episodes=  8300  avgSteps=  4.04  avgTotalReward= 0.71\n",
      "Done: episodes=  8400  avgSteps=  4.07  avgTotalReward= 0.72\n",
      "Done: episodes=  8500  avgSteps=  4.08  avgTotalReward= 0.71\n",
      "Done: episodes=  8600  avgSteps=  4.08  avgTotalReward= 0.71\n",
      "Done: episodes=  8700  avgSteps=  4.01  avgTotalReward= 0.72\n",
      "Done: episodes=  8800  avgSteps=  4.10  avgTotalReward= 0.71\n",
      "Done: episodes=  8900  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes=  9000  avgSteps=  4.08  avgTotalReward= 0.72\n",
      "Done: episodes=  9100  avgSteps=  4.02  avgTotalReward= 0.72\n",
      "Done: episodes=  9200  avgSteps=  4.09  avgTotalReward= 0.71\n",
      "Done: episodes=  9300  avgSteps=  4.01  avgTotalReward= 0.72\n",
      "Done: episodes=  9400  avgSteps=  4.04  avgTotalReward= 0.72\n",
      "Done: episodes=  9500  avgSteps=  4.02  avgTotalReward= 0.72\n",
      "Done: episodes=  9600  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes=  9700  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes=  9800  avgSteps=  4.06  avgTotalReward= 0.72\n",
      "Done: episodes=  9900  avgSteps=  4.07  avgTotalReward= 0.72\n",
      "Done: episodes= 10000  avgSteps=  4.03  avgTotalReward= 0.73\n",
      "Done: episodes= 10100  avgSteps=  4.04  avgTotalReward= 0.73\n",
      "Done: episodes= 10200  avgSteps=  4.04  avgTotalReward= 0.72\n",
      "Done: episodes= 10300  avgSteps=  4.03  avgTotalReward= 0.73\n",
      "Done: episodes= 10400  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 10500  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 10600  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 10700  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 10800  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 10900  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 11000  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 11100  avgSteps=  4.03  avgTotalReward= 0.73\n",
      "Done: episodes= 11200  avgSteps=  4.03  avgTotalReward= 0.72\n",
      "Done: episodes= 11300  avgSteps=  4.06  avgTotalReward= 0.73\n",
      "Done: episodes= 11400  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 11500  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 11600  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 11700  avgSteps=  4.03  avgTotalReward= 0.73\n",
      "Done: episodes= 11800  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 11900  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 12000  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 12100  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 12200  avgSteps=  4.00  avgTotalReward= 0.73\n",
      "Done: episodes= 12300  avgSteps=  4.01  avgTotalReward= 0.73\n",
      "Done: episodes= 12400  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 12500  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 12600  avgSteps=  4.02  avgTotalReward= 0.73\n",
      "Done: episodes= 12700  avgSteps=  4.00  avgTotalReward= 0.73\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from baselines.ppo2 import ppo2\n",
    "\n",
    "BeraterEnv.showStep = False\n",
    "BeraterEnv.showDone = False\n",
    "\n",
    "wrapped_env = DummyVecEnv([lambda: gym.make(id)])\n",
    "model = ppo2.learn(network='mlp', env=wrapped_env, total_timesteps=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtBh4c6-kQ4K"
   },
   "source": [
    "# Enjoy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "ucP0gNhhkQ4O",
    "outputId": "4f100a0e-3415-489a-c99d-818e74866648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 12732   Step:    1  S --2-> C R= 0.27 totalR= 0.27 cost= 200 customerR=1000 optimum=3000\n",
      "Episode: 12732   Step:    2  C --1-> B R= 0.25 totalR= 0.52 cost= 250 customerR=1000 optimum=3000\n",
      "Episode: 12732   Step:    3  B --0-> A R= 0.25 totalR= 0.77 cost= 250 customerR=1000 optimum=3000\n",
      "Episode: 12732   Step:    4  A --2-> S R=-0.03 totalR= 0.73 cost= 100 customerR=   0 optimum=3000\n",
      "Episode finished after 4 timesteps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "observation = wrapped_env.reset()\n",
    "state = np.zeros((1, 2*128))\n",
    "dones = np.zeros((1))\n",
    "\n",
    "BeraterEnv.showStep = True\n",
    "BeraterEnv.showDone = False\n",
    "\n",
    "for t in range(1000):\n",
    "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
    "    observation, reward, done, info = wrapped_env.step(actions)\n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5fY1da_0l15E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "berater_chh_181104.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
